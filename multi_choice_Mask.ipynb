{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-choice-Mask.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7o1D5GA7w1fB",
        "-QL20WJS7Zj6",
        "hWWljESfJW7M"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezeh-lbf/Probing-Persian-Language-Models/blob/main/multi_choice_Mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ü§ó Transformers and ü§ó Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "from IPython.display import clear_output \n",
        "!pip install -q sentencepiece\n",
        "!pip install ipywidgets\n",
        "!pip install bertviz\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akdt-iyw6BJI"
      },
      "source": [
        "!pip install tqdm --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVh5O6rv58BR"
      },
      "source": [
        "# !python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n",
        "# !pip3 install datasets\n",
        "# !python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1PA9KskwHwn",
        "cellView": "form"
      },
      "source": [
        "# @title train_model function\n",
        "\n",
        "def train_model(model, train_dataloader, validation_dataloader):\n",
        "\n",
        "  # Set the seed value all over the place to make this reproducible.\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "  criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "\n",
        "          b_input_ids =batch['input_ids'].to(device)\n",
        "          # print(b_input_ids)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "          # function and pass down the arguments. The `forward` function is \n",
        "          # documented here: \n",
        "          # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "          # The results are returned in a results object, documented here:\n",
        "          # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "          # Specifically, we'll get the loss (because we provided labels) and the\n",
        "          # \"logits\"--the model outputs prior to activation.\n",
        "          # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "          result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels) \n",
        "          loss = criterion(result, b_labels)\n",
        "          # print(loss)\n",
        "          # loss = result.loss\n",
        "          # logits = result.logits\n",
        "          # print('-----------------loss------------')\n",
        "          # print(loss)\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          # loss.requires_grad = True\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "      \n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "          # the `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          # print(batch)\n",
        "          b_input_ids = batch['input_ids'].to(device)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "\n",
        "          # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "          # output values prior to applying an activation function like the \n",
        "          # softmax.\n",
        "          loss = criterion(result, b_labels)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = result.detach().cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  return avg_val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAuE1zB5XfbT"
      },
      "source": [
        "from datasets import load_dataset, list_datasets, load_metric\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# size\n",
        "datasets = load_dataset('json', data_files='/content/size_comp_train.jsonl').shuffle()\n",
        "dataset_test = load_dataset('json', data_files='/content/size_comp_dev.jsonl').shuffle()\n",
        "\n",
        "# # num\n",
        "# datasets = load_dataset('json', data_files='/content/ant-syn_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/ant-syn_dev.jsonl').shuffle()\n",
        "\n",
        "# age\n",
        "# datasets = load_dataset('json', data_files='/content/age_comp_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/age_comp_dev.jsonl').shuffle()\n",
        "\n",
        "datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "datasets['validation'] = datasets['test']\n",
        "datasets['test'] = dataset_test['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ü§ó Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the functions `load_dataset`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1D5GA7w1fB"
      },
      "source": [
        "# Common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhZw4_xR0GmZ"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "\n",
        "# @title model checkpoint\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "\n",
        "# roberta-fa-zwnj-base\n",
        "# TODO: change Model to persian one\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jj76TVOFyV"
      },
      "source": [
        "# Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue9fbHJu4mge"
      },
      "source": [
        "##Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14PfxTi41PJH"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtlrtH2h7LS"
      },
      "source": [
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "k = tokenizer.vocab_size\n",
        "bertmodel.cpu()\n",
        "print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dni-IsXn0mTI"
      },
      "source": [
        "# # model.bert.train=False\n",
        "# for p in model.bert.parameters():\n",
        "#   p.requires_grad=False\n",
        "# for t in model.cls.parameters():\n",
        "#   t.requires_grad=True\n",
        "#   print(t.i)#.requires_grad)# print(w.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIrQPH-yYPnX",
        "cellView": "form"
      },
      "source": [
        "# @title trans_numbers,large_num2word functions\n",
        "\n",
        "def num2word(num):\n",
        "  # if num=='-':\n",
        "  #   return 'ŸÖŸÜŸÅ€å '\n",
        "  num = int(num)\n",
        "  result = ''\n",
        "  word = {1:'€å⁄©',2:'ÿØŸà',3:'ÿ≥Ÿá',4:'⁄ÜŸáÿßÿ±',5:'ŸæŸÜÿ¨',6:'ÿ¥ÿ¥',7:'ŸáŸÅÿ™',8:'Ÿáÿ¥ÿ™',9:'ŸÜŸá',10:'ÿØŸá',11:'€åÿßÿ≤ÿØŸá',12:'ÿØŸàÿßÿ≤ÿØŸá',13:'ÿ≥€åÿ≤ÿØŸá',14:'⁄ÜŸáÿßÿ±ÿØŸá',15:'ŸæÿßŸÜÿ≤ÿØŸá',16:'ÿ¥ÿßŸÜÿ≤ÿØŸá',17:'ŸáŸÅÿØŸá',18:'Ÿáÿ¨ÿØŸá',19:'ŸÜŸàÿ≤ÿØŸá',20:'ÿ®€åÿ≥ÿ™',30:'ÿ≥€å',40:'⁄ÜŸáŸÑ',50:'ŸæŸÜÿ¨ÿßŸá',60:'ÿ¥ÿµÿ™',70:'ŸáŸÅÿ™ÿßÿØ',80:'Ÿáÿ¥ÿ™ÿßÿØ',90:'ŸÜŸàÿØ',100:'ÿµÿØ',200:'ÿØŸà€åÿ≥ÿ™',300:'ÿ≥€åÿµÿØ',400:'⁄ÜŸáÿßÿ±ÿµÿØ',500:'ŸæÿßŸÜÿµÿØ',600:'ÿ¥ÿ¥ÿµÿØ',700:'ŸáŸÅÿ™ÿµÿØ',800:'Ÿáÿ¥ÿ™ÿµÿØ',900:'ŸÜŸáÿµÿØ'}\n",
        "  if num==0:\n",
        "    return 'ÿµŸÅÿ±'\n",
        "  if num<0:\n",
        "    result = 'ŸÖŸÜŸÅ€å '\n",
        "    num*=-1\n",
        "  if num <= 20:\n",
        "    result+= word[num]\n",
        "  elif num<100:\n",
        "    result+=word[10*int(num/10)]\n",
        "    if num%10>0:\n",
        "      result+=' Ÿà ' + word[num%10]\n",
        "  elif num<1000:\n",
        "    result+=word[100*int(num/100)]\n",
        "    tmp = num%100\n",
        "    if tmp>0:\n",
        "      result+=' Ÿà '+ num2word(tmp)\n",
        "\n",
        "  return result\n",
        "\n",
        "def large_num2word(num):\n",
        "  if num[0]=='-':\n",
        "    num=num[1:]\n",
        "    result='ŸÖŸÜŸÅ€å '\n",
        "  else:\n",
        "    result=''\n",
        "  suffix={0:'',1:' Ÿáÿ≤ÿßÿ±',2:' ŸÖŸÑ€åŸàŸÜ',3:' ŸÖ€åŸÑ€åÿßÿ±ÿØ',4:' ÿ™ÿ±€åŸÑ€åŸàŸÜ',5:' ÿ™ÿ±€åŸÑ€åÿßÿ±ÿØ'}\n",
        "  triples = [num[0:len(num)%3]] if len(num)%3!=0 else []\n",
        "  triples +=[num[i:i+3] for i in range(len(num)%3, len(num), 3)]\n",
        "  triples.reverse()\n",
        "  result +=num2word(triples[-1])+suffix[len(triples)-1]\n",
        "  for i in range(len(triples)-2,-1,-1):\n",
        "    n=num2word(triples[i])\n",
        "    if n!='ÿµŸÅÿ±':\n",
        "      result+=' Ÿà '+n+suffix[i]\n",
        "  return result\n",
        "\n",
        "def trans_numbers(num):\n",
        "  num = num.replace('1','€±')\n",
        "  num = num.replace('2','€≤')\n",
        "  num = num.replace('3','€≥')\n",
        "  num = num.replace('4','€¥')\n",
        "  num = num.replace('5','€µ')\n",
        "  num = num.replace('6','€∂')\n",
        "  num = num.replace('7','€∑')\n",
        "  num = num.replace('8','€∏')\n",
        "  num = num.replace('9','€π')\n",
        "  num = num.replace('0','€∞')\n",
        "  return num\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtDBTgy05S3"
      },
      "source": [
        "def predic_mask(text, option1, option2, model, tokenizer):\n",
        "  op1 = tokenizer.encode_plus(option1, return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  op2 = tokenizer.encode_plus(option2,  return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  text  =text.replace('[MASK]', tokenizer.mask_token)\n",
        "  input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "  mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "  output = model(**input)\n",
        "  logits = output.logits\n",
        "  # softmax = F.softmax(logits, dim = -1)\n",
        "  mask_word = logits[0, mask_index, :]\n",
        "  # k = tokenizer.vocab_size\n",
        "  # topk = torch.topk(mask_word, k, dim = 1)\n",
        "  p1 = mask_word[0][op1]\n",
        "  p2 = mask_word[0][op2]\n",
        "  if p1>p2:\n",
        "    return option1\n",
        "  else:\n",
        "    return option2\n",
        "  # mask_word_prob = topk[0][0]\n",
        "  # mask_word = topk[1][0]\n",
        "  # op1_index = torch.where(mask_word==op1)\n",
        "  # op2_index = torch.where(mask_word==op2)\n",
        "  # sent1 =  text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op1_index]))\n",
        "  # sent2 = text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op2_index]))\n",
        "  # if op1_index[0] > op2_index[0]:\n",
        "  #   predicted = option2\n",
        "  #   # print(sent2)\n",
        "  # else:\n",
        "  #   predicted = option1\n",
        "  #   # print(sent1)\n",
        "  # # print(predicted)\n",
        "  # return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCrfgPvbxEQ"
      },
      "source": [
        "import torch \n",
        "from torch.nn import functional as F\n",
        "text = '€å⁄© ŸÅÿ±ÿØ num1 ÿ≥ÿßŸÑŸá [MASK]‚Äåÿ™ÿ± ÿßÿ≤ €å⁄© ŸÅÿ±ÿØ num2 ÿ≥ÿßŸÑŸá ÿßÿ≥ÿ™.'\n",
        "# text = 'num1 [MASK] num2'\n",
        "options = ['ÿ¨ŸàÿßŸÜ','Ÿæ€åÿ±']\n",
        "# predic_mask(text,'ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±','⁄©Ÿà⁄Ü⁄©ÿ™ÿ±', bertmodel,tokenizer)\n",
        "eval = []\n",
        "for i in range(15,46):\n",
        "  eval.append([])\n",
        "  for j in range(18,49):\n",
        "    sent = text.replace('num1',trans_numbers(str(i)))\n",
        "    sent = sent.replace('num2',trans_numbers(str(j)))\n",
        "    p = predic_mask(sent,options[0],options[1],bertmodel,tokenizer)\n",
        "    if p == options[0]:\n",
        "      eval[i-15].append(0)\n",
        "    elif p == options[1]:\n",
        "      eval[i-15].append(1)\n",
        "    else:\n",
        "      print(p)\n",
        "      raise 1\n",
        "\n",
        "print(eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2g0Mk_Keoh2"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "ax = sns.heatmap(eval,vmin=0,vmax=1,cbar=False,xticklabels=range(18,49),yticklabels=range(15,46),square=True)#, linewidth=0.5)\n",
        "from google.colab import files\n",
        "# plt.show()\n",
        "plt.title('€å⁄© ŸÅÿ±ÿØ €±€∞ ÿ≥ÿßŸÑŸá [ÿ¨ŸàÿßŸÜ]‚Äåÿ™ÿ± ÿßÿ≤ €å⁄© ŸÅÿ±ÿØ €≤€∞ ÿ≥ÿßŸÑŸá ÿßÿ≥ÿ™.')\n",
        "plt.savefig(\"bert_farsi_example.png\")\n",
        "files.download(\"bert_farsi_example.png\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJV20NSQBWmL"
      },
      "source": [
        "# @title evaluate\n",
        "def evaluate_zero_shot(tokeninzer, model, dataset):\n",
        "  correct_answers= o1=o2=b=k= 0 \n",
        "  for obj in dataset:\n",
        "    # print(obj)\n",
        "    text = obj['question']['stem']\n",
        "    option1 = obj['question']['choices'][0]\n",
        "    option2 = obj['question']['choices'][1]\n",
        "    predicted = predic_mask(text, option1['text'], option2['text'], tokenizer=tokenizer, model=model)\n",
        "    label = obj['label']\n",
        "    # if obj['question']['choices'][label]['text'] == 'ÿ¨ŸàÿßŸÜÿ™ÿ±':\n",
        "    #   k+=1\n",
        "    # elif obj['question']['choices'][label]['text'] == 'Ÿæ€åÿ±ÿ™ÿ±':\n",
        "    #   b+=1\n",
        "    # if predicted == 'Ÿæ€åÿ±ÿ™ÿ±':\n",
        "    #   print(text.replace('[MASK]' , 'Ÿæ€åÿ±ÿ™ÿ±'))\n",
        "    if predicted == option1['text']:\n",
        "      p_label = option1['label']\n",
        "      o1 +=1\n",
        "    elif predicted == option2['text']:\n",
        "      p_label = option2['label']\n",
        "      o2+=1\n",
        "    else:\n",
        "      print(\"sth wrong\" , predicted)\n",
        "    result = int(p_label == label) \n",
        "    # print(result)\n",
        "    correct_answers +=result\n",
        "  # print(o1, o2)\n",
        "  # print('k , b: ' , k , b)\n",
        "  # print(correct_answers, len(dataset)- correct_answers)\n",
        "    # print(label, predicted)\n",
        "  metric = correct_answers/len(dataset)\n",
        "  return metric\n",
        "\n",
        "metric = evaluate_zero_shot(tokenizer, bertmodel, datasets['test'])\n",
        "print(metric, model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0_igKrkPsyK"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqHABy65Sas9"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "\n",
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "bertmodel.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d0eZm0ARkcM"
      },
      "source": [
        "for p in bertmodel.bert.parameters():\n",
        "  print(p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUpytztilCzR"
      },
      "source": [
        "# ÿßŸÖÿ®ÿ±ÿ™: ÿ≥ÿßÿπÿ™ Ÿà ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ/////////ÿ¢€åŸÜŸá ⁄Üÿ±ÿßÿ∫ ⁄Ø€åÿ™ÿßÿ±  ÿ≥ÿßÿπÿ™ ÿØŸàÿ¥ ŸÖŸàÿ®ÿß€åŸÑ ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ ÿ≥ŸÅÿßŸÑ ÿ¥ÿßŸÜŸá\n",
        "# ÿ™ŸàŸæ ÿ≥⁄©Ÿá Ÿæÿ™Ÿà ÿÆŸàÿØ⁄©ÿßÿ± ⁄©ŸÑÿßŸá\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "a = tokenizer.encode(\"⁄©Ÿà⁄Ü⁄©ÿ™ÿ± ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±  \")\n",
        "# a=tokenizer.encode('ÿπŸÑ€å ÿ≥ÿ≠ÿ± ŸÜŸÅÿ≥ ÿ®Ÿà ÿ¨€åÿ∫ ŸÜÿßÿØÿ± ŸÜÿßÿµÿ± Ÿáÿßÿ¥ŸÖ ÿ≤Ÿáÿ±ÿß ÿßŸÖ€åÿ± ⁄©ÿßŸàŸá ÿ≠ÿ≥€åŸÜ ŸÖÿ≠ŸÖÿØ ÿ≠ÿ¨ÿ™ ÿ∑ÿßŸáÿ±  ŸÖŸáÿ≥ÿß ŸÅÿßÿ∑ŸÖŸá ÿ¢€åÿØÿß ŸÖŸáÿØ€å ÿ≠ÿ≥ÿßŸÖ Ÿæÿßÿ±ÿ≥ÿß ÿ¥ÿ±Ÿà€åŸÜ ')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywlteiIZA4s2"
      },
      "source": [
        "# @title check tokenizer\n",
        "print('---------')\n",
        "a=tokenizer.encode('⁄©Ÿà⁄Ü⁄© ÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ÿ®ÿ≤ÿ±⁄Ø ÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('Ÿæ€åÿ± ÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ÿ¨ŸàÿßŸÜ ÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('⁄©Ÿà⁄Ü⁄©ÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('Ÿæ€åÿ±‚Äåÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ÿ¨ŸàÿßŸÜ‚Äåÿ™ÿ±')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ŸÜÿ®ŸàÿØ')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')\n",
        "a=tokenizer.encode('ÿ®ŸàÿØ')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤\n",
        "print('---------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S396ODVHAqul"
      },
      "source": [
        "epochs=4\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exsWYMSyVbgy"
      },
      "source": [
        "# @title fix label\n",
        "\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  for i in c:\n",
        "    if i['label']==0:\n",
        "      if i['text']=='ÿ®ŸàÿØ':\n",
        "        ex['fixed_label']=ex['label']\n",
        "      else:\n",
        "        ex['fixed_label']=1-ex['label']\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]',tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  ex={**ex,**enc}\n",
        "  return ex\n",
        "\n",
        "\n",
        "encoded_dataset = datasets.map(fix_labels)\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBikmtNABUhZ"
      },
      "source": [
        "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "# import numpy as np\n",
        "# def load_data(dataset):\n",
        "# # torch.from_numpy(np.asarray(labels)\n",
        "#   ids= torch.cat(dataset['input_ids'], dim=0)\n",
        "#   msk= torch.cat(dataset['attention_mask'], dim=0)\n",
        "#   lbl= torch.tensor(dataset['fixed_label'])\n",
        "\n",
        "#   return TensorDataset(ids,msk, lbl)\n",
        "  \n",
        "\n",
        "# # b_input_ids = batch['input_ids'].to(device)\n",
        "# # b_input_mask = batch['attention_mask'].to(device)\n",
        "# # b_labels = batch['fixed_label'].to(device)\n",
        "# # encoded_dataset['train']['input_ids'],'attention_mask','fixed_label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0bFyWSJVVXY"
      },
      "source": [
        "#@title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            encoded_dataset['train'],  # The training samples.\n",
        "            sampler = RandomSampler( encoded_dataset['train']), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            encoded_dataset['validation'], # The validation samples.\n",
        "            sampler = RandomSampler(encoded_dataset['validation']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QL0MDhR7qUF"
      },
      "source": [
        "# for step, batch in enumerate(train_dataloader):\n",
        "#   if step==0:\n",
        "\n",
        "    # print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shwUeyq0fkI8"
      },
      "source": [
        "### Bert_MC-MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1X4mIiqP4jD"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.nn import functional\n",
        "\n",
        "class Bert_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, bert_model, tokenizer, options=['ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±','⁄©Ÿà⁄Ü⁄©‚Äåÿ™ÿ±']):\n",
        "    super(Bert_MC_MLM, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.options = options\n",
        "  \n",
        "  def forward(self, ids, mask, labels):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    op1 = tokenizer.encode(self.options[0])[1]\n",
        "    op2 = tokenizer.encode(self.options[1])[1]\n",
        "    output = self.bert_model(ids,attention_mask = mask)\n",
        "    logits = output.logits\n",
        "    \n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    softmax = functional.softmax(logits, dim = -1)\n",
        "    mask_word = []\n",
        "    for i in range(len(mask_index)):\n",
        "      mask_word.append(softmax[i][mask_index[i]])\n",
        "    mask_word =  torch.stack(mask_word)\n",
        "    indices = torch.tensor([op1, op2]).to(device)\n",
        "    predicted = torch.index_select(mask_word, 1, indices)\n",
        "    # print(predicted[0][0],predicted[0][1])\n",
        "    predicted = functional.softmax(predicted)\n",
        "  \n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EljXo7alTtuq"
      },
      "source": [
        "model = Bert_MC_MLM(bertmodel,tokenizer,options=['⁄©Ÿà⁄Ü⁄©‚Äåÿ™ÿ±','ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±'])\n",
        "model.cuda()\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJp-IsNfeM5"
      },
      "source": [
        "# for step, batch in enumerate(train_dataloader):\n",
        "#   if step==0:\n",
        "#     print(batch)\n",
        "#     # print(torch.stack(batch['input_ids']))\n",
        "#     # print(torch.FloatTensor(np.ndarray(batch['input_ids'])))\n",
        "# # b_input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnGi23PpEtK6"
      },
      "source": [
        "import gc\n",
        "# del encoded_dataset\n",
        "# del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUURuwuMt73t"
      },
      "source": [
        "### training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "acc = train_model(model, train_dataloader, validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rirFQ7ZgpQEF"
      },
      "source": [
        "import torch\n",
        "c = torch.tensor([[1 , 7 , 5 ,4],[1 , 3 , 3 ,7],[1 , 3 , 7 ,4]])\n",
        "c2 = torch.tensor([[[1 , 7 , 5 ,4],[1 , 3 , 3 ,9],[1 , 3 , 7 ,4]], [[1 , 7 , 5 ,4],[1 , 3 , 3 ,2],[1 , 3 , 1 ,4]]])\n",
        "sel = torch.where(c == 7)[1]\n",
        "c2[0, sel, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QL20WJS7Zj6"
      },
      "source": [
        "## evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stHYZNmC7fzu"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgFX1ZKOOSu"
      },
      "source": [
        "#roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGaGDn6FIVk-"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "\n",
        "model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "roberta_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "roberta_model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QEmtn65IY3Y"
      },
      "source": [
        "for p in roberta_model.roberta.parameters():\n",
        "  p.requires_value=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ePQDMIIdNJ"
      },
      "source": [
        "epochs=4\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHKwTHwcIgwm"
      },
      "source": [
        "# @title fix label\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  for i in c:\n",
        "    if i['label']==0:\n",
        "      if i['text']=='⁄©Ÿà⁄Ü⁄©ÿ™ÿ±':\n",
        "        ex['fixed_label']=ex['label']\n",
        "      else:\n",
        "        ex['fixed_label']=1-ex['label']\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]', tokenizer.mask_token + ' ' + tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  \n",
        "  ex={**ex,**enc}\n",
        "  return ex\n",
        "\n",
        "encoded_dataset = datasets.map(fix_labels)\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QrMC8iJIrIg"
      },
      "source": [
        "# @title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            encoded_dataset['train'],  # The training samples.\n",
        "            sampler = RandomSampler( encoded_dataset['train']), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            encoded_dataset['validation'], # The validation samples.\n",
        "            sampler = RandomSampler(encoded_dataset['validation']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIlj4JYTOAdz"
      },
      "source": [
        "### Roberta_MC-MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2BfSDcSOUZB"
      },
      "source": [
        "c = torch.tensor([[1 , 4 , 1 , 4 ], [1 , 4 , 8 , 4 ], [1 , 4 , 4 , 4 ] ])\n",
        "c2 = torch.tensor([[1 , 4 , 1 , 4 ], [1 , 4 , 8 , 4 ], [1 , 4 , 4 , 4 ] ])\n",
        "c + c2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q2USQGqI-d4"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Roberta_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer, options=['ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±','⁄©Ÿà⁄Ü⁄©‚Äåÿ™ÿ±']):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Roberta_MC_MLM, self).__init__()\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.options = [self.tokenizer.encode(option)[1:3] for option in options]\n",
        "    print (self.options)\n",
        "  \n",
        "  def forward(self, ids, mask, labels):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    output = self.model(ids,attention_mask = mask)\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    softmax = functional.softmax(logits, dim = -1)\n",
        "    mask1_word = []\n",
        "    mask2_word = []\n",
        "    for i in range(0, len(mask_index),2):\n",
        "      mask1_word.append(softmax[i//2][mask_index[i]])\n",
        "      mask2_word.append(softmax[(i+1)//2][mask_index[i+1]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "    mask2_word =  torch.stack(mask2_word)\n",
        "    indices1 = torch.tensor([self.options[0][0],self.options[1][0] ]).to(device)\n",
        "    indices2 = torch.tensor([self.options[0][1], self.options[1][1]]).to(device)\n",
        "    predicted1 = torch.index_select(mask1_word, 1, indices1)\n",
        "    predicted2 = torch.index_select(mask2_word, 1, indices2)\n",
        "    # print(predicted1,predicted2)\n",
        "    predicted = functional.softmax(predicted1 + predicted2)\n",
        "  \n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeVfIbBgJIf1"
      },
      "source": [
        "\n",
        "model = Roberta_MC_MLM(roberta_model,tokenizer,options=['⁄©Ÿà⁄Ü⁄©ÿ™ÿ±','ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±'])\n",
        "model.cuda()\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_1RfCHHJOgq"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-z9zKwCJiKB"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-1, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        b_input_ids =batch['input_ids'].to(device)\n",
        "        # print(b_input_ids)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "        result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels) \n",
        "        loss = criterion(result, b_labels)\n",
        "        # print(loss)\n",
        "        # loss = result.loss\n",
        "        # logits = result.logits\n",
        "        # print('-----------------loss------------')\n",
        "        # print(loss)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        # loss.requires_grad = True\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    \n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        # print(batch)\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = result.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6UrInrJ3Sx"
      },
      "source": [
        "\n",
        "## evaluation\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C15-DkBh3KJa"
      },
      "source": [
        "# Breakable task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddg1nmzR35Bw"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "bertmodel.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPeCdhFcEbkd"
      },
      "source": [
        "# ÿßŸÖÿ®ÿ±ÿ™: ÿ≥ÿßÿπÿ™ Ÿà ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ/////////ÿ¢€åŸÜŸá ⁄Üÿ±ÿßÿ∫ ⁄Ø€åÿ™ÿßÿ±  ÿ≥ÿßÿπÿ™ ÿØŸàÿ¥ ŸÖŸàÿ®ÿß€åŸÑ ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ ÿ≥ŸÅÿßŸÑ ÿ¥ÿßŸÜŸá\n",
        "# ÿ™ŸàŸæ ÿ≥⁄©Ÿá Ÿæÿ™Ÿà ÿÆŸàÿØ⁄©ÿßÿ± ⁄©ŸÑÿßŸá\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "\n",
        "# from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "a=tokenizer.encode('ÿ™ŸàŸæ\",\"ÿ≥⁄©Ÿá\", \"Ÿæÿ™Ÿà\", \"ÿÆŸàÿØ⁄©ÿßÿ±\", \"⁄©ŸÑÿßŸá\",\"⁄©ÿ™ÿßÿ®\"')\n",
        "# a=tokenizer.encode('\"ÿ¢€åŸÜŸá\",\"⁄Üÿ±ÿßÿ∫\",\"⁄Ø€åÿ™ÿßÿ±\",\"ÿ¥€åÿ¥Ÿá\",\"ÿ≥ÿßÿπÿ™\",\"ÿ≥ŸÅÿßŸÑ\"')\n",
        "\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQE1FBUI4fb8"
      },
      "source": [
        "for p in roberta_model.roberta.parameters():\n",
        "  p.requres_value=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iTjtZIY4iXM"
      },
      "source": [
        "\n",
        "epochs=4\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Ca1dEl4ncQ"
      },
      "source": [
        "# @title fix label\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  for i in c:\n",
        "    if i['label']==0:\n",
        "      if i['text']=='⁄©Ÿà⁄Ü⁄©ÿ™ÿ±':\n",
        "        ex['fixed_label']=ex['label']\n",
        "      else:\n",
        "        ex['fixed_label']=1-ex['label']\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]', tokenizer.mask_token + ' ' + tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  \n",
        "  ex={**ex,**enc}\n",
        "  return ex\n",
        "\n",
        "encoded_dataset = datasets.map(fix_labels)\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPq4Wj194nKh"
      },
      "source": [
        "# @title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            encoded_dataset['train'],  # The training samples.\n",
        "            sampler = RandomSampler( encoded_dataset['train']), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            encoded_dataset['validation'], # The validation samples.\n",
        "            sampler = RandomSampler(encoded_dataset['validation']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw3IwYj54zSc"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Break_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Roberta_MC_MLM, self).__init__()\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.options = [self.tokenizer.encode(option)[1:3] for option in options]\n",
        "    print (self.options)\n",
        "  \n",
        "  def forward(self, ids, mask, labels):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    output = self.model(ids,attention_mask = mask, options)\n",
        "    options = [self.tokenizer.encode(option)[1] for option in options]\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    softmax = functional.softmax(logits, dim = -1)\n",
        "    mask1_word = []\n",
        "    mask2_word = []\n",
        "    for i in range(0, len(mask_index),2):\n",
        "      mask1_word.append(softmax[i//2][mask_index[i]])\n",
        "      mask2_word.append(softmax[(i+1)//2][mask_index[i+1]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "    mask2_word =  torch.stack(mask2_word)\n",
        "    indices1 = torch.tensor([self.options[0][0],self.options[1][0] ]).to(device)\n",
        "    indices2 = torch.tensor([self.options[0][1], self.options[1][1]]).to(device)\n",
        "    predicted1 = torch.index_select(mask1_word, 1, indices1)\n",
        "    predicted2 = torch.index_select(mask2_word, 1, indices2)\n",
        "    # print(predicted1,predicted2)\n",
        "    predicted = functional.softmax(predicted1 + predicted2)\n",
        "  \n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsKoxKWF41nY"
      },
      "source": [
        "model = Roberta_MC_MLM(roberta_model,tokenizer,options=['⁄©Ÿà⁄Ü⁄©ÿ™ÿ±','ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±'])\n",
        "model.cuda()\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hae_mdQd43rs"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjkzlTx47vp"
      },
      "source": [
        "### learning loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4tgmud83_cT"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-1, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        b_input_ids =batch['input_ids'].to(device)\n",
        "        # print(b_input_ids)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "        result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels) \n",
        "        loss = criterion(result, b_labels)\n",
        "        # print(loss)\n",
        "        # loss = result.loss\n",
        "        # logits = result.logits\n",
        "        # print('-----------------loss------------')\n",
        "        # print(loss)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        # loss.requires_grad = True\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    \n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        # print(batch)\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = result.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5TBmA3C5LAw"
      },
      "source": [
        "### evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Ft8c9F5JjU"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# multiple choice task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model to a multiple choice task, which is the task of selecting the most plausible inputs in a given selection. The dataset used here is [SWAG](https://www.aclweb.org/anthology/D18-1009/) but you can adapt the pre-processing to any other multiple choice dataset you like, or your own data. SWAG is a dataset about commonsense reasoning, where each example describes a situation then proposes four options that could go after it. \n",
        "\n",
        "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a mutiple choice head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ü§ó Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "# TODO: change tokenizer to persian one\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ü§ó Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "source": [
        "test = tokenizer(\"ÿß€åŸÜ €åŸá €±€¥ ÿ¨ŸÖŸÑŸá ŸÅÿßÿ±ÿ≥€å ÿßÿ≥ÿ™.\", \"ÿ®ÿß €åŸá ÿ¨ŸÖŸÑŸá ⁄©Ÿá ÿßÿØÿßŸÖŸá‚Äåÿ¥ ŸÖ€åÿßÿØ\")\n",
        "tokenizer.decode(test[\"input_ids\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can then write the function that will preprocess our samples. The tricky part is to put all the possible pairs of sentences in two big lists before passing them to the tokenizer, then un-flatten the result so that each example has four input ids, attentions masks, etc.\n",
        "\n",
        "When calling the `tokenizer`, we use the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FGkSTr7dDRp"
      },
      "source": [
        "# Pre process for our model\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    question_headers = examples[\"question\"]\n",
        "    sentences = [[sent['stem'].replace(\"[MASK]\", choice['text']) for choice in examples['question'][i]['choices']] for i, sent in enumerate(question_headers)]\n",
        "\n",
        "    # Flatten everything\n",
        "    sentences = sum(sentences, [])\n",
        "    # for s in sentences:\n",
        "    #   print(s)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized_examples = tokenizer(sentences, truncation=True)\n",
        "    # Un-flatten\n",
        "    # TODO: be warn that 2 is constant and stands for number of choices\n",
        "    return {k: [v[i:i+2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists of lists for each key: a list of all examples (here 5), then a list of all choices (4) and a list of input IDs (length varying here since we did not apply any padding):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JCV1gK-7ud_"
      },
      "source": [
        "examples = datasets[\"train\"][:5]vs\n",
        "features = preprocess_function(examples)\n",
        "features\n",
        "print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78tC7zrD7ueB"
      },
      "source": [
        "To check we didn't do anything group when grouping all possibilites then unflattening, let's have a look at the decoded inputs for a given example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k68scoU7ueB"
      },
      "source": [
        "idx = 2\n",
        "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8qR4VSa7ueB"
      },
      "source": [
        "We can compare it to the ground truth:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piMDCEkK7ueC"
      },
      "source": [
        "show_one(datasets[\"train\"][3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "This seems alright, so we can apply this function on all the examples in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "source": [
        "encoded_datasets = datasets.map(preprocess_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the ü§ó Datasets library to avoid spending time on this step the next time you run your notebook. The ü§ó Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ü§ó Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsLtA-jNBmWe"
      },
      "source": [
        "encoded_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYFPAK5uCVlI"
      },
      "source": [
        "show_random_elements(encoded_datasets[\"train\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our task is about mutliple choice, we use the `AutoModelForMultipleChoice` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "source": [
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    \"test-glue\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
        "\n",
        "Then we need to tell our `Trainer` how to form batches from the pre-processed inputs. We haven't done any padding yet because we will pad each batch to the maximum length inside the batch (instead of doing so with the maximum length of the whole dataset). This will be the job of the *data collator*. A data collator takes a list of examples and converts them to a batch (by, in our case, applying padding). Since there is no data collator in the library that works on our specific problem, we will write one, adapted from the `DataCollatorWithPadding`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvALJd9j7ueJ"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        if \"label\" not in features[0].keys():\n",
        "          print(features[0].keys())\n",
        "          print(features[0])\n",
        "          features[0]['label'] = 0\n",
        "          print(\"----------------------------------------\")\n",
        "        \n",
        "        # print(features[0]['label'])\n",
        "\n",
        "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
        "        # print(features[0].keys())\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0][\"input_ids\"])\n",
        "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "        \n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        \n",
        "        # Un-flatten\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        # Add back labels\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlRvwpfm7ueJ"
      },
      "source": [
        "When called on a list of examples, it will flatten all the inputs/attentions masks etc. in big lists that it will pass to the `tokenizer.pad` method. This will return a dictionary with big tensors (of shape `(batch_size * 4) x seq_length`) that we then unflatten.\n",
        "\n",
        "We can check this data collator works on a list of features, we just have to make sure to remove all features that are not inputs accepted by our model (something the `Trainer` will do automatically for us after):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-hQiXuS7ueK"
      },
      "source": [
        "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(0, 100, 5)]\n",
        "batch = DataCollatorForMultipleChoice(tokenizer)(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtPgQSX9K_qD"
      },
      "source": [
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM8UWICI7ueK"
      },
      "source": [
        "Again, all those flatten/un-flatten are sources of potential errors so let's make another sanity check on our inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsRAmLK7ueK"
      },
      "source": [
        "[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TU5jtqM7ueL"
      },
      "source": [
        "show_one(datasets[\"train\"][8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "gggtrainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()trainer.evaluate()tradfffffffieidodsifninififjidtraintrainfkffkAll good!\n",
        "\n",
        "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_predictions):\n",
        "    predictions, label_ids = eval_predictions\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_datasets[\"train\"],\n",
        "    eval_dataset=encoded_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RGGOIVr7ueO",
        "scrolled": true
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5YkwGHxWltQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OERHLFzN7ueP"
      },
      "source": [
        "Don't forget to [upload your model](https://huggingface.co/transformers/model_sharing.html) on the [ü§ó Model Hub](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeflR24e7ueP"
      },
      "source": [
        "# inputs = tokenizer.encode(\"€å⁄© ŸÅÿ±ÿØ €±€≤ ÿ≥ÿßŸÑŸá ÿßÿ≤ €å⁄© ŸÅÿ±ÿØ €±€π ÿ≥ÿßŸÑŸá ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ± ÿßÿ≥ÿ™\", return_tensors='pt')\n",
        "# outputs = model(encoded_datasets[\"validation\"])\n",
        "predict = trainer.predict(encoded_datasets['test'])\n",
        "predict.metrics\n",
        "# attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
        "# tokens = tokenizer.convert_ids_to_tokens(inputs[0]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJXJVMd6TYvo"
      },
      "source": [
        "from bertviz import head_view\n",
        "head_view(attention, tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWljESfJW7M"
      },
      "source": [
        "# TODO\n",
        "<font dir='rtl'>\n",
        "\n",
        "* ⁄Ü⁄© ⁄©ŸÜŸÖ ⁄©Ÿá ŸàŸÇÿ™€å ŸÖÿ≥⁄© ÿ¨ÿß€å ŸÖÿ™ŸÅÿßŸàÿ™€å ÿ®ÿßÿ¥Ÿá ŸáŸÖ ÿØÿ±ÿ≥ÿ™ ⁄©ÿßÿ± ŸÖ€å‚Äå⁄©ŸÜŸáÿü\n",
        "\n",
        "\n",
        "* ÿ®ÿ±ÿß€å ŸÖÿØŸÑ Ÿáÿß€å€å ⁄©Ÿá ⁄ÜŸÜÿØ ÿ™ÿß ÿ™Ÿà⁄©ŸÜ ŸÖ€å‚Äå⁄©ŸÜŸÜ €åŸá ⁄©ÿßÿ±€å ⁄©ŸÜ€åŸÖ\n",
        "*learning curve\n",
        "*test datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c2tKc4-f4-"
      },
      "source": [
        "# Notes\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz29upoU0V6M"
      },
      "source": [
        "# Trash\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9T8PFUczEEa"
      },
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFl59NX_zEEa"
      },
      "source": [
        "show_random_elements(datasets[\"train\"])\n",
        "# show_random_elements(eng_datasets[\"train\"])\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQFWjmHtzEEa"
      },
      "source": [
        "datasets[\"train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTDJZc4KzEEa"
      },
      "source": [
        "def show_one(example):\n",
        "    print(f\"Context: {example['question']['stem'].replace('[MASK]' , '------')}\")\n",
        "    print(f\"  {example['question']['choices'][0]['label']}  - {example['question']['choices'][0]['text']} \")\n",
        "    print(f\"  {example['question']['choices'][1]['label']}  - {example['question']['choices'][1]['text']} \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZluBnvaszEEb"
      },
      "source": [
        "show_one(datasets[\"train\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88eFegUgzEEb"
      },
      "source": [
        "\n",
        "\n",
        "show_one(datasets[\"train\"][15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUtpdS4NgP6x"
      },
      "source": [
        "\n",
        "# younger = tokenizer.encode_plus('ÿ¨ŸàÿßŸÜÿ™ÿ±', return_tensors = \"pt\")['input_ids'][0][1]\n",
        "# print(tokenizer.encode_plus('ÿ¨ŸàÿßŸÜÿ™ÿ±', return_tensors = \"pt\")['input_ids'][0])\n",
        "# older = tokenizer.encode_plus('Ÿæ€åÿ±ÿ™ÿ±',  return_tensors = \"pt\")['input_ids'][0][1]\n",
        "# print(tokenizer.encode_plus('Ÿæ€åÿ±ÿ™ÿ±',  return_tensors = \"pt\")['input_ids'][0])\n",
        "# text = \"€å⁄© ŸÅÿ±ÿØ €≤€≤ ÿ≥ÿßŸÑŸá ÿßÿ≤ €å⁄© ŸÅÿ±ÿØ €≤ ÿ≥ÿßŸÑŸá [MASK] ÿßÿ≥ÿ™.\"\n",
        "# text  =text.replace('[MASK]', tokenizer.mask_token)\n",
        "# input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "# mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "# print(mask_index)\n",
        "# output = model(**input)\n",
        "# logits = output.logits\n",
        "# softmax = F.softmax(logits, dim = -1)\n",
        "# mask_word = softmax[0, mask_index, :]\n",
        "\n",
        "# mask_word_prob = torch.topk(mask_word, tokenizer.vocab_size, dim = 1)[0][0]\n",
        "# mask_word = torch.topk(mask_word, tokenizer.vocab_size, dim = 1)[1][0]\n",
        "# younger_index = torch.where(mask_word==younger)\n",
        "# print(younger_index)\n",
        "# older_index = torch.where(mask_word==older)\n",
        "# print(older_index)\n",
        "# younger_sent =  text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[younger_index]))\n",
        "# print(younger_sent, mask_word_prob[younger_index])\n",
        "# if older_index[0] > younger_index[0]:\n",
        "#   predicted = '⁄©Ÿà⁄Ü⁄©ÿ™ÿ±'\n",
        "# else:\n",
        "#   predicted = 'ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±'\n",
        "# older_sent = text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[older_index]))\n",
        "# print(older_sent, mask_word_prob[older_index])\n",
        "# print(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCCPQRiKn-_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HHzPvBBoC6g"
      },
      "source": [
        "# Nolang for breakable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jugIS1Q6rOtb"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install jsonlines\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft13wubuzwVY"
      },
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "from IPython.display import clear_output \n",
        "!pip install -q sentencepiece\n",
        "\n",
        "clear_output()\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOSSaWngur2G"
      },
      "source": [
        "# from torch.nn import functional\n",
        "# import torch\n",
        "# ÿßŸÖÿ®ÿ±ÿ™: ÿ≥ÿßÿπÿ™ Ÿà ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ/////////ÿ¢€åŸÜŸá ⁄Üÿ±ÿßÿ∫ ⁄Ø€åÿ™ÿßÿ± ÿ¥€åÿ¥Ÿá  ÿ≥ÿßÿπÿ™ ÿØŸàÿ¥ ŸÖŸàÿ®ÿß€åŸÑ ÿ™ŸÑŸà€åÿ≤€åŸàŸÜ ÿ≥ŸÅÿßŸÑ ÿ¥ÿßŸÜŸá\n",
        "# ÿ™ŸàŸæ ÿ≥⁄©Ÿá Ÿæÿ™Ÿà ÿÆŸàÿØ⁄©ÿßÿ± ⁄©ŸÑÿßŸá\n",
        "# a=tokenizer.encode('ÿπŸÑ€å ÿ≥ÿ≠ÿ± ŸÜŸÅÿ≥ ÿ®Ÿà ÿ¨€åÿ∫ ŸÜÿßÿØÿ± ŸÜÿßÿµÿ± Ÿáÿßÿ¥ŸÖ ÿ≤Ÿáÿ±ÿß ÿßŸÖ€åÿ± ⁄©ÿßŸàŸá ÿ≠ÿ≥€åŸÜ ŸÖÿ≠ŸÖÿØ ÿ≠ÿ¨ÿ™ ÿ∑ÿßŸáÿ±  ŸÖŸáÿ≥ÿß ŸÅÿßÿ∑ŸÖŸá ÿ¢€åÿØÿß ŸÖŸáÿØ€å ÿ≠ÿ≥ÿßŸÖ Ÿæÿßÿ±ÿ≥ÿß ÿ¥ÿ±Ÿà€åŸÜ ')\n",
        "# \n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "model_checkpoint = 'bert-base-multilingual-cased'\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "a=tokenizer.encode('⁄©ÿ™ÿßÿ®')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#€±€¥€∞€∞€≤ €π€∂€µ€≤"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3iBPZzNwxP7"
      },
      "source": [
        "from itertools import permutations\n",
        "\n",
        "nbr=[\"ŸÖÿ≠ŸÖÿØ\", \"ÿ≠ÿ¨ÿ™\", \"ÿ∑ÿßŸáÿ±\",  \"ŸÖŸáÿ≥ÿß\", \"ŸÅÿßÿ∑ŸÖŸá\", \"ÿ¢€åÿØÿß\" ]\n",
        "br=[\"ŸÜÿßÿµÿ±\", \"Ÿáÿßÿ¥ŸÖ\", \"ÿ≤Ÿáÿ±ÿß\", \"ÿßŸÖ€åÿ±\" ,\"⁄©ÿßŸàŸá\" ,\"ÿ≠ÿ≥€åŸÜ\"]\n",
        "dataset=[]\n",
        "perm = permutations(nbr,3)\n",
        "for item1,item2, item3 in iter(perm):\n",
        "  for answer in br:\n",
        "    for i in range(4):\n",
        "      opts=[item1,item2, item3]\n",
        "      opts.insert(i,answer)\n",
        "      stem=\"ŸÖŸá‚Äåÿ±ÿÆ €å⁄© \"+opts[0]+\"ÿå €å⁄© \"+opts[1]+\"ÿå €å⁄© \"+opts[2]+\" Ÿà €å⁄© \"+opts[3]+\" ÿ±ÿß ÿßÿ≤ ŸæŸÜÿ¨ÿ±Ÿá Ÿæÿß€å€åŸÜ ŸÖ€å‚ÄåÿßŸÜÿØÿßÿ≤ÿØ. ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿß€åŸÜ⁄©Ÿá [MASK] ÿ®ÿ¥⁄©ŸÜÿØ ÿßÿ≤ ÿ®ŸÇ€åŸá ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≥ÿ™.\"\n",
        "      choices=[{\"label\":l,\"text\":t}for l,t in enumerate(opts)]\n",
        "      dataset.append({\"question\":{\"stem\":stem , \"choices\":choices},\"label\":i})\n",
        "\n",
        "\n",
        "perm = permutations(br,3)\n",
        "for item1,item2, item3 in iter(perm):\n",
        "  for answer in nbr:\n",
        "    for i in range(4):\n",
        "      opts=[item1,item2, item3]\n",
        "      opts.insert(i,answer)\n",
        "      stem = \"ŸÅÿßÿ¶ÿ≤Ÿá €å⁄© \"+opts[0]+\"ÿå €å⁄© \"+opts[1]+\"ÿå €å⁄© \"+opts[2]+\" Ÿà €å⁄© \"+opts[3]+\" ÿ±ÿß ÿßÿ≤ ŸæŸÜÿ¨ÿ±Ÿá Ÿæÿß€å€åŸÜ ŸÖ€å‚ÄåÿßŸÜÿØÿßÿ≤ÿØ. ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿß€åŸÜ⁄©Ÿá [MASK] ÿ®ÿ¥⁄©ŸÜÿØ ÿßÿ≤ ÿ®ŸÇ€åŸá ⁄©ŸÖÿ™ÿ± ÿßÿ≥ÿ™.\"\n",
        "      choices=[{\"label\":l,\"text\":t}for l,t in enumerate(opts)]\n",
        "      dataset.append({\"question\":{\"stem\":stem , \"choices\":choices},\"label\":i})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zPVGJ2AGgTh"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install jsonlines\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xie57P5aI-1C"
      },
      "source": [
        "import jsonlines\n",
        "with jsonlines.open(\"tmp.jsonl\", mode=\"w\") as writer:\n",
        "  writer.write_all(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZhpV-G4SLvr"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4dCtV1ILVup"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(\"json\", data_files=\"tmp.jsonl\").shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/ant-syn_dev.jsonl').shuffle()\n",
        "\n",
        "datasets = datasets['train'].train_test_split(760)\n",
        "datasets\n",
        "# datasets['validation'] = datasets['test']\n",
        "# datasets['test'] = dataset_test['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smR2vdwQMdwr"
      },
      "source": [
        "with open('breakable_train.jsonl','w') as f:\n",
        "  for i in datasets['train']:\n",
        "    f.write(str(i).replace(\"'\",\"\\\"\")+'\\n')\n",
        "\n",
        "with open('breakable_dev.jsonl','w') as f:\n",
        "  for i in datasets['test']:\n",
        "    f.write(str(i).replace(\"'\",\"\\\"\")+'\\n')\n",
        "    # pickle.dump(i,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ1wkyr0OFmg"
      },
      "source": [
        "dataset_test = load_dataset('json', data_files='breakable_dev.jsonl').shuffle()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhycD7LDWBC5"
      },
      "source": [
        "dataset_test['train'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQRDQYPSnsE"
      },
      "source": [
        "dataset_test['train'][0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}