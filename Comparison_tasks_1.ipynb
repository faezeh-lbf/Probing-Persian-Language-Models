{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparison tasks.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezeh-lbf/Probing-Persian-Language-Models/blob/main/Comparison_tasks_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "from IPython.display import clear_output \n",
        "!pip install -q sentencepiece\n",
        "!pip install ipywidgets\n",
        "!pip install bertviz\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akdt-iyw6BJI"
      },
      "source": [
        "!pip install tqdm --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAuE1zB5XfbT"
      },
      "source": [
        "from datasets import load_dataset, list_datasets, load_metric\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "batch_size = BATCH_SIZE\n",
        "OPTIONS_ORDER = ['بزرگتر','کوچک‌تر']\n",
        "epochs=8\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "# size\n",
        "datasets = load_dataset('json', data_files='/content/size_comp_train.jsonl').shuffle()\n",
        "dataset_test = load_dataset('json', data_files='/content/size_comp_dev.jsonl').shuffle()\n",
        "\n",
        "# # num\n",
        "# datasets = load_dataset('json', data_files='/content/ant-syn_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/ant-syn_dev.jsonl').shuffle()\n",
        "\n",
        "# age\n",
        "# datasets = load_dataset('json', data_files='/content/age_comp_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/age_comp_dev.jsonl').shuffle()\n",
        "\n",
        "datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "datasets['validation'] = datasets['test']\n",
        "datasets['test'] = dataset_test['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1D5GA7w1fB"
      },
      "source": [
        "# Common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhZw4_xR0GmZ"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "\n",
        "# @title model checkpoints\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "model_checkpoint = 'bert-base-multilingual-cased'\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "\n",
        "# roberta-fa-zwnj-base\n",
        "# TODO: change Model to persian one\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exsWYMSyVbgy"
      },
      "source": [
        "# # @title fix label\n",
        "\n",
        "# def fix_labels(ex):\n",
        "#   c=ex['question']['choices']\n",
        "#   for i in c:\n",
        "#     if i['label']==0:\n",
        "#       if i['text']== OPTIONS_ORDER[0]:\n",
        "#         ex['fixed_label']=ex['label']\n",
        "#       else:\n",
        "#         ex['fixed_label']=1-ex['label']\n",
        "#   ex['question']['stem']=ex['question']['stem'].replace('[MASK]',tokenizer.mask_token)\n",
        "#   enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "#   ex={**ex,**enc}\n",
        "#   return ex\n",
        "\n",
        "# def prepare_dataset(dataset, data_size = -1):\n",
        "#   if data_size != -1:\n",
        "#     dataset = dataset.train_test_split(data_size, seed = np.random.randint(1000))['test']\n",
        "#   encoded_dataset = dataset.map(fix_labels)\n",
        "#   encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n",
        "#   return encoded_dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k9RzdYVgWs3"
      },
      "source": [
        "# @title fix label & prepare dataset\n",
        "\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  options = []\n",
        "  for i in c:\n",
        "    options.append(i['text'])\n",
        "  options = [tokenizer.encode(option)[1] for option in options]\n",
        "  ex['fixed_label']=ex['label']\n",
        "  # print(options)\n",
        "  ex['options'] = torch.tensor(options)\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]',tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  ex={**ex,**enc}\n",
        "  sol = {}\n",
        "  cols = ['input_ids', 'attention_mask', 'fixed_label', 'options']\n",
        "  for k in ex:\n",
        "    if k in cols:\n",
        "      sol[k] = ex[k]\n",
        "  return sol\n",
        "\n",
        "def prepare_dataset(dataset, data_size = -1):\n",
        "  if data_size != -1:\n",
        "    dataset = dataset.train_test_split(data_size, seed = np.random.randint(1000))['test']\n",
        "  encoded_dataset = dataset.map(fix_labels)\n",
        "  # print(encoded_dataset[0])\n",
        "  # raise 1\n",
        "  encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "  return encoded_dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1PA9KskwHwn",
        "cellView": "form"
      },
      "source": [
        "# @title train_model function\n",
        "\n",
        "def train_model(model, train_dataloader, validation_dataloader):\n",
        "  print(\"=======================================================================\")\n",
        "\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "  criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "\n",
        "          b_input_ids =batch['input_ids'].to(device)\n",
        "          # print(b_input_ids)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "          b_options = batch['options'].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "          # function and pass down the arguments. The `forward` function is \n",
        "          # documented here: \n",
        "          # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "          # The results are returned in a results object, documented here:\n",
        "          # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "          # Specifically, we'll get the loss (because we provided labels) and the\n",
        "          # \"logits\"--the model outputs prior to activation.\n",
        "          # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "          result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        options = b_options) \n",
        "          \n",
        "          loss = criterion(result, b_labels)\n",
        "          # print(loss)\n",
        "          # loss = result.loss\n",
        "          # logits = result.logits\n",
        "          # print('-----------------loss------------')\n",
        "          # print(loss)\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          # print(loss.item())\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          # print(loss.requires_grad)\n",
        "          # loss.requires_grad = True\n",
        "          # loss = Variable(loss, requires_grad = True)\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          # raise 1\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "      # for p in params[0:5]:\n",
        "      #     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "      # par = list(model.named_parameters())\n",
        "      # params.append(par)\n",
        "      # print(params[-4])\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "      \n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "          # the `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          # print(batch)\n",
        "          b_input_ids = batch['input_ids'].to(device)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "          b_options = batch['options'].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        options = b_options)\n",
        "\n",
        "          # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "          # output values prior to applying an activation function like the \n",
        "          # softmax.\n",
        "          loss = criterion(result, b_labels)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = result.detach().cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  return avg_val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2u-adU6OGcmM"
      },
      "source": [
        "#  @title evaluate_model function\n",
        "\n",
        "def evaluate_model(model, test_dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in test_dataloader:\n",
        "\n",
        "      b_input_ids = batch['input_ids'].to(device)\n",
        "      b_input_mask = batch['attention_mask'].to(device)\n",
        "      b_labels = batch['fixed_label'].to(device)\n",
        "      b_options= batch['options'].to(device)\n",
        "      with torch.no_grad():\n",
        "        result = model(ids=b_input_ids, \n",
        "                      mask=b_input_mask, \n",
        "                      labels=b_labels,\n",
        "                      options=b_options)\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = result.detach().cpu().numpy()\n",
        "      label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "      # Calculate the accuracy for this batch of test sentences, and\n",
        "      # accumulate it over all batches.\n",
        "      total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "      \n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "  print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "  return avg_val_accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwFxx_dDv0Lm",
        "cellView": "form"
      },
      "source": [
        "# @title learning curve functions\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, batch_size=BATCH_SIZE):\n",
        "  dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = RandomSampler( dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "  return dataloader\n",
        "\n",
        "def prepare_model(model_class, model_checkpoint):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "  base_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "  # for p in bertmodel.bert.parameters():\n",
        "  #   p.requires_grad = False\n",
        "  \n",
        "  model = model_class(bertmodel, tokenizer)\n",
        "  return model\n",
        "\n",
        "  \n",
        "\n",
        "def learning_curve(model_class, model_checkpoint, datasets, learning_sizes = [64, 128, 256, 512, 1024, 2048, 4096], batch_size=BATCH_SIZE):\n",
        "  train_dataset = datasets['train']\n",
        "  val_dataset = datasets['validation']\n",
        "  val_dataset = prepare_dataset(val_dataset)\n",
        "  # print(val_dataset)\n",
        "  validation_dataloader = create_dataloader(val_dataset, batch_size)\n",
        "  test_dataloader = create_dataloader(prepare_dataset(datasets['test']), batch_size)\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for c in learning_sizes:\n",
        "    if len(datasets['train']) > c:\n",
        "      train_dataset2 = prepare_dataset(train_dataset, c)\n",
        "      train_dataloader = create_dataloader(train_dataset2, batch_size)\n",
        "      model = prepare_model(model_class, model_checkpoint)\n",
        "      model.cuda\n",
        "      train_model(model, train_dataloader, validation_dataloader)\n",
        "      eval_result = evaluate_model(model, test_dataloader)\n",
        "      results[c] = eval_result\n",
        "    else:\n",
        "      c = len(dataset['train'])\n",
        "      train_dataset2 = prepare_dataset(train_dataset, c)\n",
        "      train_dataloader = create_dataloader(train_dataset2, batch_size)\n",
        "      model = model_class()\n",
        "      model.cuda\n",
        "      train_model(model, train_dataloader, validation_dataloader)\n",
        "      eval_result = evaluate_model(model, test_dataloader)\n",
        "      results[c] = evel_result\n",
        "      return results\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jj76TVOFyV"
      },
      "source": [
        "# Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue9fbHJu4mge"
      },
      "source": [
        "##Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14PfxTi41PJH"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtlrtH2h7LS"
      },
      "source": [
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "k = tokenizer.vocab_size\n",
        "# bertmodel.cpu()\n",
        "# print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dni-IsXn0mTI"
      },
      "source": [
        "# # model.bert.train=False\n",
        "# for p in model.bert.parameters():\n",
        "#   p.requires_grad=False\n",
        "# for t in model.cls.parameters():\n",
        "#   t.requires_grad=True\n",
        "#   print(t.i)#.requires_grad)# print(w.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtDBTgy05S3"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def predic_mask(text, option1, option2, model, tokenizer):\n",
        "  op1 = tokenizer.encode_plus(option1, return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  op2 = tokenizer.encode_plus(option2,  return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  text  =text.replace('[MASK]', tokenizer.mask_token)\n",
        "  input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "  mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "  output = model(**input)\n",
        "  logits = output.logits\n",
        "  softmax = F.softmax(logits, dim = -1)\n",
        "  mask_word = softmax[0, mask_index, :]\n",
        "  k = tokenizer.vocab_size\n",
        "  topk = torch.topk(mask_word, k, dim = 1)\n",
        "  mask_word_prob = topk[0][0]\n",
        "  mask_word = topk[1][0]\n",
        "  op1_index = torch.where(mask_word==op1)\n",
        "  op2_index = torch.where(mask_word==op2)\n",
        "  sent1 =  text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op1_index]))\n",
        "  sent2 = text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op2_index]))\n",
        "  if op1_index[0] > op2_index[0]:\n",
        "    predicted = option2\n",
        "    # print(sent2)\n",
        "  else:\n",
        "    predicted = option1\n",
        "    # print(sent1)\n",
        "  # print(predicted)\n",
        "  return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJV20NSQBWmL"
      },
      "source": [
        "# @title evaluate\n",
        "def evaluate_zero_shot(tokeninzer, model, dataset):\n",
        "  correct_answers= o1=o2=b=k= 0 \n",
        "  for obj in dataset:\n",
        "    # print(obj)\n",
        "    text = obj['question']['stem']\n",
        "    option1 = obj['question']['choices'][0]\n",
        "    option2 = obj['question']['choices'][1]\n",
        "    predicted = predic_mask(text, option1['text'], option2['text'], tokenizer=tokenizer, model=model)\n",
        "    label = obj['label']\n",
        "    # if obj['question']['choices'][label]['text'] == 'جوانتر':\n",
        "    #   k+=1\n",
        "    # elif obj['question']['choices'][label]['text'] == 'پیرتر':\n",
        "    #   b+=1\n",
        "    # if predicted == 'پیرتر':\n",
        "    #   print(text.replace('[MASK]' , 'پیرتر'))\n",
        "    if predicted == option1['text']:\n",
        "      p_label = option1['label']\n",
        "      o1 +=1\n",
        "    elif predicted == option2['text']:\n",
        "      p_label = option2['label']\n",
        "      o2+=1\n",
        "    else:\n",
        "      print(\"sth wrong\" , predicted)\n",
        "    result = int(p_label == label) \n",
        "    # print(result)\n",
        "    correct_answers +=result\n",
        "  # print(o1, o2)\n",
        "  # print('k , b: ' , k , b)\n",
        "  # print(correct_answers, len(dataset)- correct_answers)\n",
        "    # print(label, predicted)\n",
        "  metric = correct_answers/len(dataset)\n",
        "  return metric\n",
        "\n",
        "metric = evaluate_zero_shot(tokenizer, bertmodel, datasets['test'])\n",
        "print(metric, model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0_igKrkPsyK"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqHABy65Sas9"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "\n",
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "bertmodel.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUpytztilCzR"
      },
      "source": [
        "# امبرت: ساعت و تلویزیون/////////آینه چراغ گیتار  ساعت دوش موبایل تلویزیون سفال شانه\n",
        "# توپ سکه پتو خودکار کلاه\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "# from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# a=tokenizer.encode('بزرگتر')\n",
        "# for i in a:\n",
        "#   print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywlteiIZA4s2",
        "cellView": "form"
      },
      "source": [
        "# @title check tokenizer\n",
        "print('---------')\n",
        "a=tokenizer.encode('وز')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('بزرگ تر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('پیر تر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('جوان تر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('کوچکتر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('بزرگتر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('پیر‌تر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('جوان‌تر')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('نبود')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')\n",
        "a=tokenizer.encode('بود')\n",
        "for i in a:\n",
        "  print(tokenizer.decode(i),i)#۱۴۰۰۲ ۹۶۵۲\n",
        "print('---------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S396ODVHAqul"
      },
      "source": [
        "epochs=4\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBikmtNABUhZ"
      },
      "source": [
        "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "# import numpy as np\n",
        "# def load_data(dataset):\n",
        "# # torch.from_numpy(np.asarray(labels)\n",
        "#   ids= torch.cat(dataset['input_ids'], dim=0)\n",
        "#   msk= torch.cat(dataset['attention_mask'], dim=0)\n",
        "#   lbl= torch.tensor(dataset['fixed_label'])\n",
        "\n",
        "#   return TensorDataset(ids,msk, lbl)\n",
        "  \n",
        "\n",
        "# # b_input_ids = batch['input_ids'].to(device)\n",
        "# # b_input_mask = batch['attention_mask'].to(device)\n",
        "# # b_labels = batch['fixed_label'].to(device)\n",
        "# # encoded_dataset['train']['input_ids'],'attention_mask','fixed_label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0bFyWSJVVXY",
        "cellView": "form"
      },
      "source": [
        "# @title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            encoded_dataset['train'],  # The training samples.\n",
        "            sampler = RandomSampler( encoded_dataset['train']), # Select batches randomly\n",
        "            batch_size = BATCH_SIZE # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            encoded_dataset['validation'], # The validation samples.\n",
        "            sampler = RandomSampler(encoded_dataset['validation']), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QL0MDhR7qUF"
      },
      "source": [
        "# for step, batch in enumerate(train_dataloader):\n",
        "#   if step==0:\n",
        "\n",
        "    # print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shwUeyq0fkI8"
      },
      "source": [
        "### Bert_MC-MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOte1DThagC2"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Bert_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Bert_MC_MLM, self).__init__()\n",
        "    for p in model.bert.parameters():\n",
        "      p.requres_value=False\n",
        "\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "  \n",
        "  def forward(self, ids, mask, labels, options):\n",
        "    output = self.model(ids,attention_mask = mask)\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    mask1_word = []\n",
        "    for i in range(0, len(mask_index)):\n",
        "      mask1_word.append(logits[i][mask_index[i]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "    indices1 = torch.tensor(options).to(device)\n",
        "    predicted1 = []\n",
        "    for i in range(len(mask1_word)):\n",
        "      x = [mask1_word[i][j] for j in indices1[i]]\n",
        "      x = torch.stack(x)\n",
        "      predicted1.append(x)\n",
        "    predicted1 = torch.stack(predicted1)\n",
        "\n",
        "    predicted = functional.softmax(predicted1)\n",
        "\n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1X4mIiqP4jD"
      },
      "source": [
        "\n",
        "# import torch\n",
        "# from torch.nn import functional\n",
        "\n",
        "# class Bert_MC_MLM(torch.nn.Module):\n",
        "#   def __init__(self, bert_model, tokenizer, options=OPTIONS_ORDER):\n",
        "#     # for p in bert_model.bert.parameters():\n",
        "#     #   p.requires_grad = False\n",
        "#     super(Bert_MC_MLM, self).__init__()\n",
        "#     self.bert_model = bert_model\n",
        "#     self.tokenizer = tokenizer\n",
        "#     self.options = options\n",
        "  \n",
        "#   def forward(self, ids, mask, labels):\n",
        "#     # print(\"__________-----------------------______________\")\n",
        "#     op1 = tokenizer.encode(self.options[0])[1]\n",
        "#     op2 = tokenizer.encode(self.options[1])[1]\n",
        "#     output = self.bert_model(ids,attention_mask = mask)\n",
        "#     logits = output.logits\n",
        "    \n",
        "#     mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "#     # softmax = functional.softmax(logits, dim = -1)\n",
        "#     # print(softmax[0])\n",
        "#     mask_word = []\n",
        "#     # print(len(mask_index))\n",
        "#     for i in range(0, len(mask_index)):\n",
        "#       mask_word.append(logits[i][mask_index[i]])\n",
        "#     mask_word =  torch.stack(mask_word)\n",
        "#     indices = torch.tensor([op1, op2]).to(device)\n",
        "#     predicted = torch.index_select(mask_word, 1, indices)\n",
        "#     # print(predicted[0][0],predicted[0][1])\n",
        "#     predicted = functional.softmax(predicted)\n",
        "  \n",
        "#     return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYifJV3XNwpv"
      },
      "source": [
        "# @title CURVE\n",
        "curve_info= learning_curve(Bert_MC_MLM, model_checkpoint, datasets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgBqDqwE54V8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# curve_info = c\n",
        "plt.plot(list(curve_info.keys()), list(curve_info.values()), 'ro')\n",
        "plt.plot(list(curve_info.keys()), list(curve_info.values()), 'r-')\n",
        "plt.xscale('log')\n",
        "\n",
        "from google.colab import files\n",
        "plt.savefig(\"abc.png\")\n",
        "files.download(\"abc.png\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EljXo7alTtuq"
      },
      "source": [
        "model = Bert_MC_MLM(bertmodel, tokenizer, options=OPTIONS_ORDER )\n",
        "model.cuda()\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJp-IsNfeM5"
      },
      "source": [
        "# for step, batch in enumerate(train_dataloader):\n",
        "#   if step==0:\n",
        "#     print(batch)\n",
        "#     # print(torch.stack(batch['input_ids']))\n",
        "#     # print(torch.FloatTensor(np.ndarray(batch['input_ids'])))\n",
        "# # b_input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnGi23PpEtK6"
      },
      "source": [
        "import gc\n",
        "# del encoded_dataset\n",
        "# del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqUTij0f4uQs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUURuwuMt73t"
      },
      "source": [
        "### training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "acc = train_model(model, train_dataloader, validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rirFQ7ZgpQEF"
      },
      "source": [
        "import torch\n",
        "c = torch.tensor([[1 , 7 , 5 ,4],[1 , 3 , 3 ,7],[1 , 3 , 7 ,4]])\n",
        "c2 = torch.tensor([[[1 , 7 , 5 ,4],[1 , 3 , 3 ,9],[1 , 3 , 7 ,4]], [[1 , 7 , 5 ,4],[1 , 3 , 3 ,2],[1 , 3 , 1 ,4]]])\n",
        "sel = torch.where(c == 7)[1]\n",
        "c2[0, sel, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QL20WJS7Zj6"
      },
      "source": [
        "## evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stHYZNmC7fzu"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgFX1ZKOOSu"
      },
      "source": [
        "#roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGaGDn6FIVk-"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "\n",
        "model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "roberta_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "roberta_model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QEmtn65IY3Y"
      },
      "source": [
        "for p in roberta_model.roberta.parameters():\n",
        "  p.requires_value=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ePQDMIIdNJ"
      },
      "source": [
        "epochs=4\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHKwTHwcIgwm"
      },
      "source": [
        "# @title fix label\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  for i in c:\n",
        "    if i['label']==0:\n",
        "      if i['text']=='کوچکتر':\n",
        "        ex['fixed_label']=ex['label']\n",
        "      else:\n",
        "        ex['fixed_label']=1-ex['label']\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]', tokenizer.mask_token + ' ' + tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  \n",
        "  ex={**ex,**enc}\n",
        "  return ex\n",
        "\n",
        "encoded_dataset = datasets.map(fix_labels)\n",
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QrMC8iJIrIg"
      },
      "source": [
        "# @title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            encoded_dataset['train'],  # The training samples.\n",
        "            sampler = RandomSampler( encoded_dataset['train']), # Select batches randomly\n",
        "            batch_size = BATCH_SIZE # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            encoded_dataset['validation'], # The validation samples.\n",
        "            sampler = RandomSampler(encoded_dataset['validation']), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIlj4JYTOAdz"
      },
      "source": [
        "### Roberta_MC-MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2BfSDcSOUZB"
      },
      "source": [
        "c = torch.tensor([[1 , 4 , 1 , 4 ], [1 , 4 , 8 , 4 ], [1 , 4 , 4 , 4 ] ])\n",
        "c2 = torch.tensor([[1 , 4 , 1 , 4 ], [1 , 4 , 8 , 4 ], [1 , 4 , 4 , 4 ] ])\n",
        "c + c2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q2USQGqI-d4"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Roberta_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer, options=OPTIONS_ORDER):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Roberta_MC_MLM, self).__init__()\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.options = [self.tokenizer.encode(option)[1:3] for option in options]\n",
        "    print (self.options)\n",
        "  \n",
        "  def forward(self, ids, mask, labels):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    output = self.model(ids,attention_mask = mask)\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    softmax = functional.softmax(logits, dim = -1)\n",
        "    mask1_word = []\n",
        "    mask2_word = []\n",
        "    for i in range(0, len(mask_index),2):\n",
        "      mask1_word.append(softmax[i//2][mask_index[i]])\n",
        "      mask2_word.append(softmax[(i+1)//2][mask_index[i+1]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "    mask2_word =  torch.stack(mask2_word)\n",
        "    indices1 = torch.tensor([self.options[0][0],self.options[1][0] ]).to(device)\n",
        "    indices2 = torch.tensor([self.options[0][1], self.options[1][1]]).to(device)\n",
        "    predicted1 = torch.index_select(mask1_word, 1, indices1)\n",
        "    predicted2 = torch.index_select(mask2_word, 1, indices2)\n",
        "    # print(predicted1,predicted2)\n",
        "    predicted = functional.softmax(predicted1 + predicted2)\n",
        "  \n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeVfIbBgJIf1"
      },
      "source": [
        "\n",
        "model = Roberta_MC_MLM(roberta_model,tokenizer,options=OPTIONS_ORDER)\n",
        "model.cuda()\n",
        "# clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_1RfCHHJOgq"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-z9zKwCJiKB"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-1, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        b_input_ids =batch['input_ids'].to(device)\n",
        "        # print(b_input_ids)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "        result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels) \n",
        "        loss = criterion(result, b_labels)\n",
        "        # print(loss)\n",
        "        # loss = result.loss\n",
        "        # logits = result.logits\n",
        "        # print('-----------------loss------------')\n",
        "        # print(loss)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        # loss.requires_grad = True\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    \n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        # print(batch)\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = result.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6UrInrJ3Sx"
      },
      "source": [
        "\n",
        "## evaluation\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovBzMX33G_xH"
      },
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "\n",
        "*   fix label is dependent on the task ( بود و نبود و بزرگتر کوچک‌تر)\n",
        "*   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5CProyPQd88"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}