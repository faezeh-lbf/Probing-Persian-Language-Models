{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparison Tasks.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezeh-lbf/Probing-Persian-Language-Models/blob/main/Comparison_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "! pip install git+https://github.com/huggingface/datasets.git\n",
        "from IPython.display import clear_output \n",
        "!pip install -q sentencepiece\n",
        "# !pip install ipywidgets\n",
        "# !pip install bertviz\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akdt-iyw6BJI"
      },
      "source": [
        "!pip install tqdm --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAuE1zB5XfbT"
      },
      "source": [
        "from datasets import load_dataset, list_datasets, load_metric\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "batch_size = BATCH_SIZE\n",
        "# OPTIONS_ORDER = ['بزرگتر', 'کوچکتر']\n",
        "\n",
        "\n",
        "# size\n",
        "dataset_name = '/content/age_comp_train'\n",
        "# datasets = load_dataset('json', data_files='/content/size_comp_train.jsonl').shuffle()\n",
        "test_dataset = load_dataset('json', data_files='/content/age_comp_dev_nolang.jsonl').shuffle()['train']\n",
        "# print(test_dataset)\n",
        "# datasets = load_dataset('json', data_files='/content/breakable_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/breakable_dev.jsonl').shuffle()\n",
        "\n",
        "# # num\n",
        "# datasets = load_dataset('json', data_files='/content/ant-syn_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/ant-syn_dev.jsonl').shuffle()\n",
        "\n",
        "# age\n",
        "# datasets = load_dataset('json', data_files='/content/age_comp_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/age_comp_dev.jsonl').shuffle()\n",
        "\n",
        "# datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "# datasets['validation'] = datasets['test']\n",
        "# datasets['test'] = dataset_test['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chdD8oMPoTFR"
      },
      "source": [
        "# nolang datasets\n",
        "\n",
        "from datasets import load_dataset, list_datasets, load_metric\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "batch_size = BATCH_SIZE\n",
        "# size\n",
        "# nolang_dataset_name = '/content/size_comp_train_nolang'\n",
        "datasets = load_dataset('json', data_files='/content/age_comp_train.jsonl').shuffle()\n",
        "# nolang_dataset_test = load_dataset('json', data_files='/content/nolang_size_comp_dev.jsonl').shuffle()\n",
        "# datasets = load_dataset('json', data_files='/content/nolang_breakable_train.jsonl').shuffle()\n",
        "dataset_test = load_dataset('json', data_files='/content/age_comp_dev.jsonl').shuffle()\n",
        "\n",
        "# # num\n",
        "# datasets = load_dataset('json', data_files='/content/ant-syn_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/ant-syn_dev.jsonl').shuffle()\n",
        "\n",
        "# age\n",
        "# datasets = load_dataset('json', data_files='/content/age_comp_train.jsonl').shuffle()\n",
        "# dataset_test = load_dataset('json', data_files='/content/age_comp_dev.jsonl').shuffle()\n",
        "\n",
        "datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "datasets['validation'] = datasets['test']\n",
        "datasets['test'] = dataset_test['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpmP0QaODKpE"
      },
      "source": [
        "datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1D5GA7w1fB"
      },
      "source": [
        "# Common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "\n",
        "# @title model checkpoints\n",
        "# eng_model_checkpoint = \"bert-base-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "model_checkpoint = 'bert-base-multilingual-cased'\n",
        "\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "\n",
        "# roberta-fa-zwnj-base\n",
        "\n",
        "# TODO: change Model to persian one\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BGR6FD4qyUQ",
        "cellView": "form"
      },
      "source": [
        "# @title learning curve functions\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, batch_size=BATCH_SIZE):\n",
        "  dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = RandomSampler( dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "  return dataloader\n",
        "\n",
        "def prepare_model(model_class, model_checkpoint):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "  base_model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "  \n",
        "  model = model_class(base_model, tokenizer)\n",
        "  model.cuda()\n",
        "  return model\n",
        "\n",
        "  \n",
        "\n",
        "def learning_curve(model_class, model_checkpoint, test_dataset, dataset_name, learning_sizes = [64, 128, 256, 512, 1024, 2048, 4096], batch_size=BATCH_SIZE):\n",
        "  # train_dataset = datasets['train']\n",
        "  # val_dataset = datasets['validation']\n",
        "  # val_dataset = prepare_dataset(val_dataset)\n",
        "  test_dataset = test_dataset.map(fix_labels)\n",
        "  test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "  # validation_dataloader = create_dataloader(val_dataset, batch_size)\n",
        "  test_dataloader = create_dataloader(test_dataset, batch_size)\n",
        "\n",
        "  results = {}\n",
        "  model = prepare_model(model_class, model_checkpoint)\n",
        "  eval_result = evaluate_model(model, test_dataloader)\n",
        "  results[0] = eval_result\n",
        "  \n",
        "  for c in learning_sizes:\n",
        "    train_dataset2, val_dataset2 = prepare_dataset(dataset_name, c)\n",
        "    train_dataloader = create_dataloader(train_dataset2, batch_size)\n",
        "    val_dataloader = create_dataloader(val_dataset2, batch_size)\n",
        "    model = prepare_model(model_class, model_checkpoint)\n",
        "    train_model(model, train_dataloader, val_dataloader)\n",
        "    eval_result = evaluate_model(model, test_dataloader)\n",
        "    results[c] = eval_result\n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exsWYMSyVbgy"
      },
      "source": [
        "# @title fix label\n",
        "\n",
        "def fix_labels(ex):\n",
        "  c=ex['question']['choices']\n",
        "  options = []\n",
        "  for i in c:\n",
        "    options.append(i['text'])\n",
        "  options = [tokenizer.encode(option)[1] for option in options]\n",
        "  ex['fixed_label']=ex['label']\n",
        "  # print(options)\n",
        "  ex['options'] = torch.tensor(options)\n",
        "  ex['question']['stem']=ex['question']['stem'].replace('[MASK]',tokenizer.mask_token)\n",
        "  enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  ex={**ex,**enc}\n",
        "  sol = {}\n",
        "  cols = ['input_ids', 'attention_mask', 'fixed_label', 'options']\n",
        "  for k in ex:\n",
        "    if k in cols:\n",
        "      sol[k] = ex[k]\n",
        "  return sol\n",
        "\n",
        "def prepare_dataset(file_name, data_size = -1):\n",
        "  # print(encoded_dataset[0])\n",
        "  # raise 1\n",
        "  # t = [0,0]\n",
        "  # for m in encoded_dataset:\n",
        "  #   t[m['fixed_label']]+=1\n",
        "\n",
        "  # print(t)\n",
        "  \n",
        "    # print(dataset)\n",
        "  #   d1 = []\n",
        "  #   i = 0\n",
        "  #   while len(d1)<data_size/2:\n",
        "  #     if encoded_dataset[i]['fixed_label'] == 1:\n",
        "  #       d1.append(encoded_dataset[i])\n",
        "  #     i+=1\n",
        "  #   # print(d1)\n",
        "  #   d0 = []\n",
        "  #   i = 0\n",
        "  #   while len(d0)<data_size/2:\n",
        "  #     if encoded_dataset[i]['fixed_label'] == 0:\n",
        "  #       d0.append(encoded_dataset[i])\n",
        "  #     i+=1\n",
        "  # d0.extend(d1)\n",
        "  # encoded_dataset = load_dataset('list',d0)\n",
        "\n",
        "  if data_size != -1:\n",
        "    file_name = file_name + '_' + str(data_size) + '.jsonl'\n",
        "    datasets = load_dataset('json', data_files=file_name).shuffle()\n",
        "\n",
        "    datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "    datasets['validation'] = datasets['test']\n",
        "  else:    \n",
        "    file_name = file_name +  '.jsonl'\n",
        "    datasets = load_dataset('json', data_files=file_name).shuffle()\n",
        "    datasets = datasets['train'].train_test_split(test_size=0.1)\n",
        "    # dataset = dataset.train_test_split(data_size, seed = np.random.randint(1000))['test']\n",
        "  val_encoded_dataset = datasets['test'].map(fix_labels)\n",
        "  train_encoded_dataset = datasets['train'].map(fix_labels)\n",
        "  # encoded_dataset = encoded_dataset.shuffle()\n",
        "  val_encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "  train_encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "  return train_encoded_dataset, val_encoded_dataset\n",
        "\n",
        "# prepare_dataset(datasets['test'],64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1WOLmD8q_tR"
      },
      "source": [
        "# @title train_model function - real\n",
        "\n",
        "def train_model(model, train_dataloader, validation_dataloader):\n",
        "  print(\"=======================================================================\")\n",
        "\n",
        "  seed_val = 42\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "  criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "\n",
        "          b_input_ids =batch['input_ids'].to(device)\n",
        "          # print(b_input_ids)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "          b_options = batch['options'].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "          # function and pass down the arguments. The `forward` function is \n",
        "          # documented here: \n",
        "          # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "          # The results are returned in a results object, documented here:\n",
        "          # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "          # Specifically, we'll get the loss (because we provided labels) and the\n",
        "          # \"logits\"--the model outputs prior to activation.\n",
        "          # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "          result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        options = b_options) \n",
        "          \n",
        "          loss = criterion(result, b_labels)\n",
        "          # print(loss)\n",
        "          # loss = result.loss\n",
        "          # logits = result.logits\n",
        "          # print('-----------------loss------------')\n",
        "          # print(loss)\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          # print(loss.item())\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          # print(loss.requires_grad)\n",
        "          # loss.requires_grad = True\n",
        "          # loss = Variable(loss, requires_grad = True)\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          # raise 1\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "      # for p in params[0:5]:\n",
        "      #     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "      # par = list(model.named_parameters())\n",
        "      # params.append(par)\n",
        "      # print(params[-4])\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "      \n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "          # the `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          # print(batch)\n",
        "          b_input_ids = batch['input_ids'].to(device)\n",
        "          b_input_mask = batch['attention_mask'].to(device)\n",
        "          b_labels = batch['fixed_label'].to(device)\n",
        "          b_options = batch['options'].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              result = model(ids=b_input_ids, \n",
        "                        mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        options = b_options)\n",
        "\n",
        "          # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "          # output values prior to applying an activation function like the \n",
        "          # softmax.\n",
        "          loss = criterion(result, b_labels)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = result.detach().cpu().numpy()\n",
        "          label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  return avg_val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57E_IxWpq_tS",
        "cellView": "form"
      },
      "source": [
        "#  @title evaluate_model function\n",
        "\n",
        "def evaluate_model(model, test_dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in test_dataloader:\n",
        "\n",
        "      b_input_ids = batch['input_ids'].to(device)\n",
        "      b_input_mask = batch['attention_mask'].to(device)\n",
        "      b_labels = batch['fixed_label'].to(device)\n",
        "      b_options= batch['options'].to(device)\n",
        "      with torch.no_grad():\n",
        "        result = model(ids=b_input_ids, \n",
        "                      mask=b_input_mask, \n",
        "                      labels=b_labels,\n",
        "                      options=b_options)\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = result.detach().cpu().numpy()\n",
        "      label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "      # Calculate the accuracy for this batch of test sentences, and\n",
        "      # accumulate it over all batches.\n",
        "      total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "      \n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "  print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "  return avg_val_accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRnl3Vd-Q6C4"
      },
      "source": [
        "\n",
        "# @title Compare1_MC_MLM\n",
        "import torch\n",
        "from torch.nn import functional\n",
        "\n",
        "\n",
        "class Compare1_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Compare1_MC_MLM, self).__init__()\n",
        "    for p in model.bert.parameters():\n",
        "      p.requires_grad=False\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    # self.options = [self.tokenizer.encode(option)[1:3] for option in options]\n",
        "    # print (self.options)\n",
        "  \n",
        "  def forward(self, ids, mask, labels, options):\n",
        "\n",
        "    output = self.model(ids,attention_mask = mask)\n",
        "    # options = [self.tokenizer.encode(option)[1] for option in options]\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # softmax = functional.softmax(logits, dim = -1)\n",
        "    # print(softmax.shape)\n",
        "    mask1_word = []\n",
        "    # mask2_word = []\n",
        "    for i in range(0, len(mask_index)):\n",
        "      mask1_word.append(logits[i][mask_index[i]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "\n",
        "    indices1 = torch.tensor(options).to(device)\n",
        "\n",
        "    predicted1 = []\n",
        "    for i in range(len(mask1_word)):\n",
        "      # print(\"mask, \" , mask1_word[i])\n",
        "      x = [mask1_word[i][j] for j in indices1[i]]\n",
        "      x = torch.stack(x)\n",
        "      # print(x.shape)\n",
        "      predicted1.append(x)\n",
        "\n",
        "    predicted1 = torch.stack(predicted1)\n",
        "\n",
        "    predicted = functional.softmax(predicted1)\n",
        "\n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92-JaGmXkejT"
      },
      "source": [
        "## curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iTjtZIY4iXM"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "epochs=4\n",
        "device = torch.device(\"cuda\")\n",
        "# model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT8amM9iDd7y"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "bertmodel.cuda()\n",
        "\n",
        "# model = Compare1_MC_MLM(bertmodel,tokenizer)\n",
        "# model.cuda()\n",
        "# clear_output()\n",
        "# train_model(model, train_dataloader, validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIrQPH-yYPnX"
      },
      "source": [
        "# @title trans_numbers,large_num2word functions\n",
        "\n",
        "def num2word(num):\n",
        "  # if num=='-':\n",
        "  #   return 'منفی '\n",
        "  num = int(num)\n",
        "  result = ''\n",
        "  word = {1:'یک',2:'دو',3:'سه',4:'چهار',5:'پنج',6:'شش',7:'هفت',8:'هشت',9:'نه',10:'ده',11:'یازده',12:'دوازده',13:'سیزده',14:'چهارده',15:'پانزده',16:'شانزده',17:'هفده',18:'هجده',19:'نوزده',20:'بیست',30:'سی',40:'چهل',50:'پنجاه',60:'شصت',70:'هفتاد',80:'هشتاد',90:'نود',100:'صد',200:'دویست',300:'سیصد',400:'چهارصد',500:'پانصد',600:'ششصد',700:'هفتصد',800:'هشتصد',900:'نهصد'}\n",
        "  if num==0:\n",
        "    return 'صفر'\n",
        "  if num<0:\n",
        "    result = 'منفی '\n",
        "    num*=-1\n",
        "  if num <= 20:\n",
        "    result+= word[num]\n",
        "  elif num<100:\n",
        "    result+=word[10*int(num/10)]\n",
        "    if num%10>0:\n",
        "      result+=' و ' + word[num%10]\n",
        "  elif num<1000:\n",
        "    result+=word[100*int(num/100)]\n",
        "    tmp = num%100\n",
        "    if tmp>0:\n",
        "      result+=' و '+ num2word(tmp)\n",
        "\n",
        "  return result\n",
        "\n",
        "def large_num2word(num):\n",
        "  if num[0]=='-':\n",
        "    num=num[1:]\n",
        "    result='منفی '\n",
        "  else:\n",
        "    result=''\n",
        "  suffix={0:'',1:' هزار',2:' ملیون',3:' میلیارد',4:' تریلیون',5:' تریلیارد'}\n",
        "  triples = [num[0:len(num)%3]] if len(num)%3!=0 else []\n",
        "  triples +=[num[i:i+3] for i in range(len(num)%3, len(num), 3)]\n",
        "  triples.reverse()\n",
        "  result +=num2word(triples[-1])+suffix[len(triples)-1]\n",
        "  for i in range(len(triples)-2,-1,-1):\n",
        "    n=num2word(triples[i])\n",
        "    if n!='صفر':\n",
        "      result+=' و '+n+suffix[i]\n",
        "  return result\n",
        "\n",
        "def trans_numbers(num):\n",
        "  num = num.replace('1','۱')\n",
        "  num = num.replace('2','۲')\n",
        "  num = num.replace('3','۳')\n",
        "  num = num.replace('4','۴')\n",
        "  num = num.replace('5','۵')\n",
        "  num = num.replace('6','۶')\n",
        "  num = num.replace('7','۷')\n",
        "  num = num.replace('8','۸')\n",
        "  num = num.replace('9','۹')\n",
        "  num = num.replace('0','۰')\n",
        "  return num\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtDBTgy05S3"
      },
      "source": [
        "def predic_mask(text, option1, option2, model, tokenizer):\n",
        "  op1 = tokenizer.encode_plus(option1, return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  op2 = tokenizer.encode_plus(option2,  return_tensors = \"pt\")['input_ids'][0][1]\n",
        "  text  =text.replace('[MASK]', tokenizer.mask_token)\n",
        "  input = tokenizer.encode_plus(text, return_tensors = \"pt\").to(device)\n",
        "  mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "  output = model(**input)\n",
        "  logits = output.logits\n",
        "  # softmax = F.softmax(logits, dim = -1)\n",
        "  mask_word = logits[0, mask_index, :]\n",
        "  # k = tokenizer.vocab_size\n",
        "  # topk = torch.topk(mask_word, k, dim = 1)\n",
        "  p1 = mask_word[0][op1]\n",
        "  p2 = mask_word[0][op2]\n",
        "  if p1>p2:\n",
        "    return option1\n",
        "  else:\n",
        "    return option2\n",
        "  # mask_word_prob = topk[0][0]\n",
        "  # mask_word = topk[1][0]\n",
        "  # op1_index = torch.where(mask_word==op1)\n",
        "  # op2_index = torch.where(mask_word==op2)\n",
        "  # sent1 =  text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op1_index]))\n",
        "  # sent2 = text.replace(tokenizer.mask_token, tokenizer.decode(mask_word[op2_index]))\n",
        "  # if op1_index[0] > op2_index[0]:\n",
        "  #   predicted = option2\n",
        "  #   # print(sent2)\n",
        "  # else:\n",
        "  #   predicted = option1\n",
        "  #   # print(sent1)\n",
        "  # # print(predicted)\n",
        "  # return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCrfgPvbxEQ"
      },
      "source": [
        "import torch \n",
        "from torch.nn import functional as F\n",
        "# text = 'یک فرد num1 ساله [MASK] از یک فرد num2 ساله است.'\n",
        "text = 'num1 [MASK] num2'\n",
        "options = ['بیشتر','کمتر']\n",
        "# predic_mask(text,'بزرگتر','کوچکتر', bertmodel,tokenizer)\n",
        "eval = []\n",
        "for i in range(15,46):\n",
        "  eval.append([])\n",
        "  for j in range(18,49):\n",
        "    sent = text.replace('num1',trans_numbers(str(i)))\n",
        "    sent = sent.replace('num2',trans_numbers(str(j)))\n",
        "    p = predic_mask(sent,options[0],options[1],bertmodel,tokenizer)\n",
        "    if p == options[0]:\n",
        "      eval[i-15].append(0)\n",
        "    elif p == options[1]:\n",
        "      eval[i-15].append(1)\n",
        "    else:\n",
        "      print(p)\n",
        "      raise 1\n",
        "\n",
        "print(eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2g0Mk_Keoh2"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "ax = sns.heatmap(eval,vmin=0,vmax=1,cbar=False,xticklabels=range(18,49),yticklabels=range(15,46),square=True)#, linewidth=0.5)\n",
        "from google.colab import files\n",
        "# plt.show()\n",
        "# plt.title('یک فرد ده ساله [جوان]‌تر از یک فرد بیست ساله است.')\n",
        "plt.savefig(\"mbert_num_fa.png\")\n",
        "files.download(\"mbert_num_fa.png\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scBwP5WxJLsh"
      },
      "source": [
        "tokenizer.encode('بزرگتر')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S60IvpkIZ56a"
      },
      "source": [
        "curve_info = learning_curve(Compare1_MC_MLM, model_checkpoint, test_dataset, dataset_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNdAscPxjZEv"
      },
      "source": [
        "nolang_curve_info = learning_curve(Compare1_MC_MLM, model_checkpoint, nolang_test_datasets, nolang_dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2B3fLoXqB-3"
      },
      "source": [
        "curve_info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v-XiHEtZ5dr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qycg8piQBeZI"
      },
      "source": [
        "nolang_curve_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt94i5xwbOM8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# age alphabet nolang bert\n",
        "# nolang_curve_info = {0: 0.37421875,10: 0.37421875,\n",
        "#  64: 0.487890625,\n",
        "#  128: 0.47890625,\n",
        "#  256: 0.383203125,\n",
        "#  512: 0.452734375,\n",
        "#  1024: 0.457421875,\n",
        "#  2048: 0.492578125,\n",
        "#  4096: 0.6}\n",
        "\n",
        "#  #age alphabet albert\n",
        "# curve_info = {0:0.581640625,\n",
        "#     10: 0.581640625,\n",
        "#  64: 0.675,\n",
        "#  128: 0.683984375,\n",
        "#  256: 0.70546875,\n",
        "#  512: 0.6175,\n",
        "#  1024: 0.6703125,\n",
        "#  2048: 0.621171875,\n",
        "#  4096: 0.679296875}\n",
        "\n",
        "\n",
        "plt.plot(list(curve_info.keys()), list(curve_info.values()), 'bo')\n",
        "plt.plot(list(curve_info.keys()), list(curve_info.values()), 'b-',label = 'age comparison')\n",
        "\n",
        "# plt.plot(list(nolang_curve_info.keys()), list(nolang_curve_info.values()), 'ro')\n",
        "# plt.plot(list(nolang_curve_info.keys()), list(nolang_curve_info.values()), 'r--',label = 'age comparison nolang')\n",
        "\n",
        "plt.xscale('log', basex=2)\n",
        "plt.legend()\n",
        "from google.colab import files\n",
        "plt.savefig(\"num_mbert.png\")\n",
        "files.download(\"num_mbert.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C15-DkBh3KJa"
      },
      "source": [
        "# Models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw3IwYj54zSc"
      },
      "source": [
        "\n",
        "# @title Compare1_MC_MLM\n",
        "import torch\n",
        "from torch.nn import functional\n",
        "\n",
        "\n",
        "class Compare1_MC_MLM(torch.nn.Module):\n",
        "  def __init__(self, model, tokenizer):\n",
        "    # print(\"__________-----------------------______________\")\n",
        "    super(Compare1_MC_MLM, self).__init__()\n",
        "    for p in model.albert.parameters():\n",
        "      p.requires_grad=False\n",
        "    self.model = model\n",
        "    self.model.cuda()\n",
        "    self.tokenizer = tokenizer\n",
        "    # self.options = [self.tokenizer.encode(option)[1:3] for option in options]\n",
        "    # print (self.options)\n",
        "  \n",
        "  def forward(self, ids, mask, labels, options):\n",
        "\n",
        "    output = self.model(ids,attention_mask = mask)\n",
        "    # options = [self.tokenizer.encode(option)[1] for option in options]\n",
        "    logits = output.logits\n",
        "\n",
        "    mask_index = torch.where(ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # softmax = functional.softmax(logits, dim = -1)\n",
        "    # print(softmax.shape)\n",
        "    mask1_word = []\n",
        "    # mask2_word = []\n",
        "    for i in range(0, len(mask_index)):\n",
        "      mask1_word.append(logits[i][mask_index[i]])\n",
        "    mask1_word =  torch.stack(mask1_word)\n",
        "\n",
        "    indices1 = torch.tensor(options).to(device)\n",
        "\n",
        "    predicted1 = []\n",
        "    for i in range(len(mask1_word)):\n",
        "      # print(\"mask, \" , mask1_word[i])\n",
        "      x = [mask1_word[i][j] for j in indices1[i]]\n",
        "      x = torch.stack(x)\n",
        "      # print(x.shape)\n",
        "      predicted1.append(x)\n",
        "\n",
        "    predicted1 = torch.stack(predicted1)\n",
        "\n",
        "    predicted = functional.softmax(predicted1)\n",
        "\n",
        "    return predicted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddg1nmzR35Bw"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.nn import functional\n",
        "import torch\n",
        "model_checkpoint = 'HooshvareLab/roberta-fa-zwnj-base'\n",
        "# model_checkpoint = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "# model_checkpoint = 'bert-base-multilingual-cased'\n",
        "# model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "\n",
        "bertmodel.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQE1FBUI4fb8"
      },
      "source": [
        "# for p in bertmodel.bert.parameters():\n",
        "#   p.requires_grad=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Ca1dEl4ncQ"
      },
      "source": [
        "# # @title fix label\n",
        "# def fix_labels(ex):\n",
        "#   c=ex['question']['choices']\n",
        "#   # for i in c:\n",
        "#   #   if i['label']==0:\n",
        "#   #     if i['text']=='کوچکتر':\n",
        "#   #       ex['fixed_label']=ex['label']\n",
        "#   #     else:\n",
        "#   #       ex['fixed_label']=1-ex['label']\n",
        "#   ex['fixed_label']=ex['label']\n",
        "  \n",
        "#   ex['question']['stem']=ex['question']['stem'].replace('[MASK]', tokenizer.mask_token + ' ' + tokenizer.mask_token)\n",
        "#   enc=tokenizer.encode_plus(ex['question']['stem'],truncation=True,padding='max_length', max_length=64)\n",
        "  \n",
        "#   ex={**ex,**enc}\n",
        "#   return ex\n",
        "\n",
        "# encoded_dataset = datasets.map(fix_labels)\n",
        "# encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label'])\n",
        "val_encoded_dataset = datasets['validation'].map(fix_labels)\n",
        "train_encoded_dataset = datasets['train'].map(fix_labels)\n",
        "# encoded_dataset = encoded_dataset.shuffle()\n",
        "val_encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "train_encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fixed_label', 'options'])\n",
        "# return train_encoded_dataset, val_encoded_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPq4Wj194nKh"
      },
      "source": [
        "# @title dataset loader\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "encoded_dataset = fix_labels(datasets)\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_encoded_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_encoded_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_encoded_dataset, # The validation samples.\n",
        "            sampler = RandomSampler(val_encoded_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            encoded_dataset['test'], # The validation samples.\n",
        "            sampler = SequentialSampler(encoded_dataset['test']), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP5Cf7I-V9vU"
      },
      "source": [
        "# for i , j in enumerate(test_dataloader):\n",
        "#   print(i,j)\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUmDdt77f7em"
      },
      "source": [
        "for p in model.model.lm_head.parameters():\n",
        "    print(p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsKoxKWF41nY"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bertmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint, return_dict = True)\n",
        "bertmodel.cuda()\n",
        "\n",
        "model = Compare1_MC_MLM(bertmodel,tokenizer)\n",
        "model.cuda()\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGB11RJ3qker"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "# for p in params[0:5]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "for p in params:\n",
        "  print(p[-1].grad)\n",
        "  break\n",
        "# params[-1][1].grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hae_mdQd43rs"
      },
      "source": [
        "# print(loss.grad)\n",
        "\n",
        "print(loss.retain_grad())\n",
        "# # print(params[-1][1].grad)\n",
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9QhuyZuQm6b"
      },
      "source": [
        "# result = model(ids=b_input_ids, \n",
        "#                 mask=b_input_mask, \n",
        "#                 labels=b_labels,\n",
        "#                 options = b_options) \n",
        "\n",
        "loss = criterion(result, b_labels)\n",
        "# # print(loss)\n",
        "# # loss = result.loss\n",
        "# # logits = result.logits\n",
        "# # print('-----------------loss------------')\n",
        "# # print(loss)\n",
        "\n",
        "# # Accumulate the training loss over all of the batches so that we can\n",
        "# # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "# # single value; the `.item()` function just returns the Python value \n",
        "# # from the tensor.\n",
        "# print(loss.item())\n",
        "# # total_train_loss += loss.item()\n",
        "\n",
        "\n",
        "# # Perform a backward pass to calculate the gradients.\n",
        "# # print(loss.requires_grad)\n",
        "# # loss.requires_grad = True\n",
        "# # loss = Variable(loss, requires_grad = True)\n",
        "# loss.backward()\n",
        "\n",
        "# # Clip the norm of the gradients to 1.0.\n",
        "# # This is to help prevent the \"exploding gradients\" problem.\n",
        "# torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjkzlTx47vp"
      },
      "source": [
        "\n",
        "### learning loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4tgmud83_cT"
      },
      "source": [
        "import random, time\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        b_input_ids =batch['input_ids'].to(device)\n",
        "        # print(b_input_ids)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        # # token_type_ids = torch.stack(batch['token_type_ids']).to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "        b_options = batch['options'].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        # print(b_input_ids.shape,b_labels.shape,b_input_mask.shape)\n",
        "        result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       options = b_options) \n",
        "        \n",
        "        loss = criterion(result, b_labels)\n",
        "        # print(loss)\n",
        "        # loss = result.loss\n",
        "        # logits = result.logits\n",
        "        # print('-----------------loss------------')\n",
        "        # print(loss)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        # print(loss.item())\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        # print(loss.requires_grad)\n",
        "        # loss.requires_grad = True\n",
        "        # loss = Variable(loss, requires_grad = True)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        # raise 1\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    # for p in params[0:5]:\n",
        "    #     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    # par = list(model.named_parameters())\n",
        "    # params.append(par)\n",
        "    # print(params[-4])\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    \n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        # print(batch)\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['fixed_label'].to(device)\n",
        "        b_options = batch['options'].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(ids=b_input_ids, \n",
        "                       mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       options = b_options)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = result.detach().cpu().numpy()\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5VH1uhy3Qcl"
      },
      "source": [
        "if params[0] != params[1]:\n",
        "  print('1')\n",
        "if params[0] != params[2]:\n",
        "  print('2')\n",
        "if params[0] != params[3]:\n",
        "  print('3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5TBmA3C5LAw"
      },
      "source": [
        "### evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Ft8c9F5JjU"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_input_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['fixed_label'].to(device)\n",
        "    b_options = batch['options'].to(device)\n",
        "    with torch.no_grad():\n",
        "      result = model(ids=b_input_ids, \n",
        "                    mask=b_input_mask, \n",
        "                    labels=b_labels,\n",
        "                     options = b_options)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = result.detach().cpu().numpy()\n",
        "    label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "print(\"  Accuracy on test set: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvApwdRTU-nj"
      },
      "source": [
        "# TODO\n",
        "\n",
        "\n",
        "*   pass tokenizer to fix label (it uses it)\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2s0Ejz1aY9M"
      },
      "source": [
        "# Notes\n",
        "## Run Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZcKW2XFkrqb"
      },
      "source": [
        "# size bert\n",
        "curve = {64: 0.376953125,\n",
        " 128: 0.35234375,\n",
        " 256: 0.362109375,\n",
        " 512: 0.48046875,\n",
        " 1024: 0.4265625,\n",
        " 2048: 0.487890625,\n",
        " 4096: 0.545703125}\n",
        " \n",
        " # size nolang bert\n",
        " curve = {64: 0.507421875,\n",
        " 128: 0.52421875,\n",
        " 256: 0.5015625,\n",
        " 512: 0.501171875,\n",
        " 1024: 0.53671875,\n",
        " 2048: 0.61171875,\n",
        " 4096: 0.674609375}\n",
        "\n",
        "# size albert\n",
        "curve = {64: 0.47734375,\n",
        " 128: 0.49296875,\n",
        " 256: 0.49296875,\n",
        " 512: 0.5203125,\n",
        " 1024: 0.523046875,\n",
        " 2048: 0.56171875,\n",
        " 4096: 0.580078125}\n",
        "\n",
        " # size nolang albert\n",
        " curve = {64: 0.498046875,\n",
        " 128: 0.48828125,\n",
        " 256: 0.537109375,\n",
        " 512: 0.545703125,\n",
        " 1024: 0.576953125,\n",
        " 2048: 0.559375,\n",
        " 4096: 0.54765625}\n",
        "\n",
        " # age alphabet bert\n",
        "curve_info = {0: 0.494140625,\n",
        " 64: 0.4875,\n",
        " 128: 0.358984375,\n",
        " 256: 0.468359375,\n",
        " 512: 0.38359375,\n",
        " 1024: 0.460546875,\n",
        " 2048: 0.3625,\n",
        " 4096: 0.4}\n",
        "\n",
        "# age alphabet nolang bert\n",
        "nolang_curve_info = {0: 0.37421875,\n",
        " 64: 0.487890625,\n",
        " 128: 0.47890625,\n",
        " 256: 0.383203125,\n",
        " 512: 0.452734375,\n",
        " 1024: 0.457421875,\n",
        " 2048: 0.492578125,\n",
        " 4096: 0.6}\n",
        "\n",
        " #age alphabet albert\n",
        " curve_info = {0: 0.581640625,\n",
        " 64: 0.675,\n",
        " 128: 0.683984375,\n",
        " 256: 0.70546875,\n",
        " 512: 0.5875,\n",
        " 1024: 0.6703125,\n",
        " 2048: 0.601171875,\n",
        " 4096: 0.679296875}\n",
        "\n",
        " #age alphabet nolang albert\n",
        "nolang_curve_info = {0: 0.49296875,\n",
        " 64: 0.5875,\n",
        " 128: 0.509375,\n",
        " 256: 0.44296875,\n",
        " 512: 0.509375,\n",
        " 1024: 0.455859375,\n",
        " 2048: 0.448046875,\n",
        " 4096: 0.466796875}\n",
        "\n",
        "#age mbert \n",
        " curve_info = {0: 0.489453125,\n",
        " 64: 0.49296875,\n",
        " 128: 0.49296875,\n",
        " 256: 0.49296875,\n",
        " 512: 0.49296875,\n",
        " 1024: 0.49609375,\n",
        " 2048: 0.543359375,\n",
        " 4096: 0.574609375}\n",
        "\n",
        "\n",
        "# age mbert nolang\n",
        "nolang_curve_info = {0: 0.409765625,\n",
        " 64: 0.371484375,\n",
        " 128: 0.503515625,\n",
        " 256: 0.43203125,\n",
        " 512: 0.441015625,\n",
        " 1024: 0.509375,\n",
        " 2048: 0.596875,\n",
        " 4096: 0.64375}\n",
        "\n",
        "\n",
        "#age alph mbert\n",
        " curve_info ={0: 0.489453125,\n",
        " 64: 0.565625,\n",
        " 128: 0.51328125,\n",
        " 256: 0.602734375,\n",
        " 512: 0.53359375,\n",
        " 1024: 0.53046875,\n",
        " 2048: 0.55859375,\n",
        " 4096: 0.551171875}\n",
        "\n",
        "\n",
        "# age alph mbert nolang\n",
        "nolang_curve_info = {0: 0.409765625,\n",
        " 64: 0.40859375,\n",
        " 128: 0.38515625,\n",
        " 256: 0.40859375,\n",
        " 512: 0.4296875,\n",
        " 1024: 0.505859375,\n",
        " 2048: 0.599609375,\n",
        " 4096: 0.64375}\n",
        "\n",
        "\n",
        " # size mbert\n",
        "curve_info ={0: 0.494140625,\n",
        " 64: 0.4953125,\n",
        " 128: 0.4953125,\n",
        " 256: 0.493359375,\n",
        " 512: 0.498828125,\n",
        " 1024: 0.49609375,\n",
        " 2048: 0.54296875,\n",
        " 4096: 0.496484375}\n",
        "\n",
        "\n",
        "# size mbert nolang\n",
        "nolang_curve_info = {0: 0.5453125,\n",
        " 64: 0.54296875,\n",
        " 128: 0.551953125,\n",
        " 256: 0.523046875,\n",
        " 512: 0.5125,\n",
        " 1024: 0.55078125,\n",
        " 2048: 0.475390625,\n",
        " 4096: 0.4765625}\n",
        "\n",
        "# num mbert \n",
        "curve_info = {0: 0.494140625,\n",
        " 64: 0.46875,\n",
        " 128: 0.4828125,\n",
        " 256: 0.50078125,\n",
        " 512: 0.4875,\n",
        " 1024: 0.525,\n",
        " 2048: 0.558203125,\n",
        " 4096: 0.57578125}\n",
        "\n",
        "# num mbert nolang\n",
        "nolang_curve_info = {0: 0.510546875,\n",
        " 64: 0.509375,\n",
        " 128: 0.525,\n",
        " 256: 0.5171875,\n",
        " 512: 0.588671875,\n",
        " 1024: 0.65546875,\n",
        " 2048: 0.732421875,\n",
        " 4096: 0.723828125}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17GcsH0YmTad"
      },
      "source": [
        "\n",
        "def diff(l1,l2):\n",
        "  res = {}\n",
        "  for k in l1:\n",
        "    res[k] = max(0,l1[k]-l2[k])\n",
        "  return res\n",
        "    \n",
        "def ws(l):\n",
        "  w = [0.23, 0.2, 0.17, 0.14, 0.11, 0.08, 0.07]\n",
        "  count = 0\n",
        "  sum = 0\n",
        "  i = 0\n",
        "  for c, acc in l.items():\n",
        "    sum += acc*w[i]\n",
        "    count += w[i]\n",
        "    i+=1\n",
        "  return sum/count\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}